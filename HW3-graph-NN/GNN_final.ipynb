{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlCNZsWlOSfP"
   },
   "source": [
    "# Practical Session on Graph Neural Networks\n",
    "\n",
    "**by Matthieu Nastorg**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0RMEfCaEehI"
   },
   "source": [
    "## **PART 1 : CODING** (8/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5Lp4PasOby4"
   },
   "source": [
    "### Install Pytorch Geometric\n",
    "\n",
    "To handle graph data, we use the library Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "\n",
    "*   If you use _Google Colab_, simply run the following cell to install Pytorch Geometric (**advised**).\n",
    "*   If you plan using your _own environment_, follow the documentation to install Pytorch Geometric : https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html and skip the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "buW3eZmrj1N2",
    "outputId": "bd9bb6d9-99c9-4e00-c316-1561a2e60b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.0%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.0+pt113cu116\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.16%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.16+pt113cu116\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.0%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-cluster) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.0+pt113cu116\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
      "Collecting torch-spline-conv\n",
      "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_spline_conv-1.2.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (874 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m874.7/874.7 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
      "Successfully installed torch-spline-conv-1.2.1+pt113cu116\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.25.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.1)\n",
      "Collecting psutil>=5.8.0\n",
      "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.2/280.2 KB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.14)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (4.0.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=3c601fdd93e93cc8d921a03d9909e0aa974529c61d641733af3704c5ab47a8c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: psutil, torch-geometric\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "Successfully installed psutil-5.9.4 torch-geometric-2.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "psutil"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## INSTALL TORCH GEOMETRIC ##################\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/ \n",
    "#####################################################\n",
    "import torch \n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04JPKXjDclTj"
   },
   "source": [
    "### Import required packages\n",
    "\n",
    "Run the following cell to import all required packages. This cell **must not** be modified.\n",
    "\n",
    "To significantly accelerate your training, it is advised to use GPU. Using Google Colab, you need to activate it : \n",
    "\n",
    "*   Edit --> Notebook Setting --> Hardware accelerator --> GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXGDmBMYgA_x"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "################## PACKAGES #########################\n",
    "#####################################################\n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as graphnn\n",
    "from sklearn.metrics import f1_score\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UvCNG8FgdS-"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We use the Protein-Protein Interaction (PPI) network dataset which includes:\n",
    "- 20 graphs for training \n",
    "- 2 graphs for validation\n",
    "- 2 graphs for testing\n",
    "\n",
    "One graph of the PPI dataset has on average 2372 nodes. Each node:\n",
    "- 50 features : positional gene sets / motif gene / immunological signatures ...\n",
    "- 121 (binary) labels : gene ontology sets (way to classify gene products like proteins).\n",
    "\n",
    "**This problem aims to predict, for a given PPI graph, the correct node's labels**.\n",
    "\n",
    "**It is a node (multi-level) classification task** (trained using supervised learning). \n",
    "\n",
    "For your curiosity, more details information on the dataset and some applications:\n",
    "- https://cs.stanford.edu/~jure/pubs/pathways-psb18.pdf\n",
    "- https://arxiv.org/abs/1707.04638\n",
    "\n",
    "To understand how a graph data is implemented in Pytorch Geometric, refer to : https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwdNhvzVNkZB",
    "outputId": "0a7b071c-d79a-47e4-9614-de493bcd662f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/ppi.zip\n",
      "Extracting ./ppi.zip\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train dataset:  20\n",
      "Number of samples in the val dataset:  2\n",
      "Number of samples in the test dataset:  2\n",
      "Output of one sample from the train dataset:  Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
      "Edge_index :\n",
      "tensor([[   0,    0,    0,  ..., 1744, 1745, 1749],\n",
      "        [ 372, 1101,  766,  ..., 1745, 1744, 1739]])\n",
      "Number of features per node:  50\n",
      "Number of classes per node:  121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### LOAD DATASETS\n",
    "\n",
    "BATCH_SIZE = 2 \n",
    "\n",
    "# Train Dataset\n",
    "train_dataset = PPI(root=\"\", split='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "# Val Dataset\n",
    "val_dataset = PPI(root=\"\", split='val')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "# Test Dataset\n",
    "test_dataset = PPI(root=\"\", split='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Number of features and classes\n",
    "n_features, n_classes = train_dataset[0].x.shape[1], train_dataset[0].y.shape[1]\n",
    "\n",
    "print(\"Number of samples in the train dataset: \", len(train_dataset))\n",
    "print(\"Number of samples in the val dataset: \", len(test_dataset))\n",
    "print(\"Number of samples in the test dataset: \", len(test_dataset))\n",
    "print(\"Output of one sample from the train dataset: \", train_dataset[0])\n",
    "print(\"Edge_index :\")\n",
    "print(train_dataset[0].edge_index)\n",
    "print(\"Number of features per node: \", n_features)\n",
    "print(\"Number of classes per node: \", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiCcn9qeO6Nm"
   },
   "source": [
    "### Define a basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RjzEiJ-eVot"
   },
   "source": [
    "Here we define a very simple Graph Neural Network model which will be used as our baseline. This model consists of three graph convolutional layers (from https://arxiv.org/pdf/1609.02907.pdf). The first two layers computes 256 features, followed by an ELU activation function. The last layer is used for (multi-level) classification task, computing 121 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Km-GN1aMpd_"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "################## MODEL ############################\n",
    "#####################################################\n",
    "class BasicGraphModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.graphconv1 = graphnn.GCNConv(input_size, hidden_size)\n",
    "        self.graphconv2 = graphnn.GCNConv(hidden_size, hidden_size)\n",
    "        self.graphconv3 = graphnn.GCNConv(hidden_size, output_size)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.graphconv1(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv2(x, edge_index)\n",
    "        x = self.elu(x)\n",
    "        x = self.graphconv3(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ekk0TrOktOB"
   },
   "source": [
    "Next we construct the function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4lneoadMxqy"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############## TRAIN FUNCTION #######################\n",
    "#####################################################\n",
    "def train(model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader):\n",
    "\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "    # loop over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch + 1, loss_data))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTd9OqaelLni"
   },
   "source": [
    "Next function is designed to evaluate the performance of the model, computing the F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PajZzg5zM7V1"
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "############### TEST FUNCTION #######################\n",
    "#####################################################\n",
    "def evaluate(model, loss_fcn, device, dataloader):\n",
    "\n",
    "    score_list_batch = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index)\n",
    "        loss_test = loss_fcn(output, batch.y)\n",
    "        predict = np.where(output.detach().cpu().numpy() >= 0, 1, 0)\n",
    "        score = f1_score(batch.y.cpu().numpy(), predict, average=\"micro\")\n",
    "        score_list_batch.append(score)\n",
    "\n",
    "    return np.array(score_list_batch).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilgopwMlpsu"
   },
   "source": [
    "Let's train this model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xkqo7e0gNACE",
    "outputId": "315b2d06-c472-4821-f1a3-6189b2cc52e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Device:  cuda\n",
      "Epoch 00001 | Loss: 0.6363\n",
      "F1-Score: 0.4364\n",
      "Epoch 00002 | Loss: 0.5820\n",
      "Epoch 00003 | Loss: 0.5623\n",
      "Epoch 00004 | Loss: 0.5563\n",
      "Epoch 00005 | Loss: 0.5510\n",
      "Epoch 00006 | Loss: 0.5458\n",
      "F1-Score: 0.4863\n",
      "Epoch 00007 | Loss: 0.5412\n",
      "Epoch 00008 | Loss: 0.5370\n",
      "Epoch 00009 | Loss: 0.5337\n",
      "Epoch 00010 | Loss: 0.5310\n",
      "Epoch 00011 | Loss: 0.5282\n",
      "F1-Score: 0.5271\n",
      "Epoch 00012 | Loss: 0.5255\n",
      "Epoch 00013 | Loss: 0.5230\n",
      "Epoch 00014 | Loss: 0.5206\n",
      "Epoch 00015 | Loss: 0.5183\n",
      "Epoch 00016 | Loss: 0.5160\n",
      "F1-Score: 0.5356\n",
      "Epoch 00017 | Loss: 0.5138\n",
      "Epoch 00018 | Loss: 0.5117\n",
      "Epoch 00019 | Loss: 0.5097\n",
      "Epoch 00020 | Loss: 0.5077\n",
      "Epoch 00021 | Loss: 0.5059\n",
      "F1-Score: 0.5238\n",
      "Epoch 00022 | Loss: 0.5041\n",
      "Epoch 00023 | Loss: 0.5024\n",
      "Epoch 00024 | Loss: 0.5003\n",
      "Epoch 00025 | Loss: 0.4981\n",
      "Epoch 00026 | Loss: 0.4962\n",
      "F1-Score: 0.5321\n",
      "Epoch 00027 | Loss: 0.4947\n",
      "Epoch 00028 | Loss: 0.4929\n",
      "Epoch 00029 | Loss: 0.4910\n",
      "Epoch 00030 | Loss: 0.4891\n",
      "Epoch 00031 | Loss: 0.4871\n",
      "F1-Score: 0.5464\n",
      "Epoch 00032 | Loss: 0.4852\n",
      "Epoch 00033 | Loss: 0.4839\n",
      "Epoch 00034 | Loss: 0.4822\n",
      "Epoch 00035 | Loss: 0.4805\n",
      "Epoch 00036 | Loss: 0.4789\n",
      "F1-Score: 0.5542\n",
      "Epoch 00037 | Loss: 0.4779\n",
      "Epoch 00038 | Loss: 0.4767\n",
      "Epoch 00039 | Loss: 0.4759\n",
      "Epoch 00040 | Loss: 0.4747\n",
      "Epoch 00041 | Loss: 0.4733\n",
      "F1-Score: 0.5664\n",
      "Epoch 00042 | Loss: 0.4726\n",
      "Epoch 00043 | Loss: 0.4721\n",
      "Epoch 00044 | Loss: 0.4704\n",
      "Epoch 00045 | Loss: 0.4692\n",
      "Epoch 00046 | Loss: 0.4669\n",
      "F1-Score: 0.5729\n",
      "Epoch 00047 | Loss: 0.4645\n",
      "Epoch 00048 | Loss: 0.4637\n",
      "Epoch 00049 | Loss: 0.4621\n",
      "Epoch 00050 | Loss: 0.4605\n",
      "Epoch 00051 | Loss: 0.4594\n",
      "F1-Score: 0.5822\n",
      "Epoch 00052 | Loss: 0.4585\n",
      "Epoch 00053 | Loss: 0.4576\n",
      "Epoch 00054 | Loss: 0.4564\n",
      "Epoch 00055 | Loss: 0.4558\n",
      "Epoch 00056 | Loss: 0.4551\n",
      "F1-Score: 0.5906\n",
      "Epoch 00057 | Loss: 0.4548\n",
      "Epoch 00058 | Loss: 0.4544\n",
      "Epoch 00059 | Loss: 0.4542\n",
      "Epoch 00060 | Loss: 0.4542\n",
      "Epoch 00061 | Loss: 0.4568\n",
      "F1-Score: 0.5644\n",
      "Epoch 00062 | Loss: 0.4548\n",
      "Epoch 00063 | Loss: 0.4538\n",
      "Epoch 00064 | Loss: 0.4506\n",
      "Epoch 00065 | Loss: 0.4500\n",
      "Epoch 00066 | Loss: 0.4492\n",
      "F1-Score: 0.5899\n",
      "Epoch 00067 | Loss: 0.4480\n",
      "Epoch 00068 | Loss: 0.4470\n",
      "Epoch 00069 | Loss: 0.4456\n",
      "Epoch 00070 | Loss: 0.4451\n",
      "Epoch 00071 | Loss: 0.4446\n",
      "F1-Score: 0.5746\n",
      "Epoch 00072 | Loss: 0.4442\n",
      "Epoch 00073 | Loss: 0.4446\n",
      "Epoch 00074 | Loss: 0.4439\n",
      "Epoch 00075 | Loss: 0.4438\n",
      "Epoch 00076 | Loss: 0.4444\n",
      "F1-Score: 0.5728\n",
      "Epoch 00077 | Loss: 0.4435\n",
      "Epoch 00078 | Loss: 0.4437\n",
      "Epoch 00079 | Loss: 0.4445\n",
      "Epoch 00080 | Loss: 0.4422\n",
      "Epoch 00081 | Loss: 0.4415\n",
      "F1-Score: 0.5569\n",
      "Epoch 00082 | Loss: 0.4397\n",
      "Epoch 00083 | Loss: 0.4398\n",
      "Epoch 00084 | Loss: 0.4410\n",
      "Epoch 00085 | Loss: 0.4435\n",
      "Epoch 00086 | Loss: 0.4440\n",
      "F1-Score: 0.6069\n",
      "Epoch 00087 | Loss: 0.4426\n",
      "Epoch 00088 | Loss: 0.4428\n",
      "Epoch 00089 | Loss: 0.4403\n",
      "Epoch 00090 | Loss: 0.4383\n",
      "Epoch 00091 | Loss: 0.4358\n",
      "F1-Score: 0.5903\n",
      "Epoch 00092 | Loss: 0.4355\n",
      "Epoch 00093 | Loss: 0.4343\n",
      "Epoch 00094 | Loss: 0.4345\n",
      "Epoch 00095 | Loss: 0.4359\n",
      "Epoch 00096 | Loss: 0.4363\n",
      "F1-Score: 0.6203\n",
      "Epoch 00097 | Loss: 0.4356\n",
      "Epoch 00098 | Loss: 0.4341\n",
      "Epoch 00099 | Loss: 0.4338\n",
      "Epoch 00100 | Loss: 0.4332\n",
      "Epoch 00101 | Loss: 0.4315\n",
      "F1-Score: 0.5817\n",
      "Epoch 00102 | Loss: 0.4310\n",
      "Epoch 00103 | Loss: 0.4297\n",
      "Epoch 00104 | Loss: 0.4301\n",
      "Epoch 00105 | Loss: 0.4287\n",
      "Epoch 00106 | Loss: 0.4285\n",
      "F1-Score: 0.6221\n",
      "Epoch 00107 | Loss: 0.4276\n",
      "Epoch 00108 | Loss: 0.4263\n",
      "Epoch 00109 | Loss: 0.4256\n",
      "Epoch 00110 | Loss: 0.4253\n",
      "Epoch 00111 | Loss: 0.4249\n",
      "F1-Score: 0.6073\n",
      "Epoch 00112 | Loss: 0.4249\n",
      "Epoch 00113 | Loss: 0.4241\n",
      "Epoch 00114 | Loss: 0.4235\n",
      "Epoch 00115 | Loss: 0.4239\n",
      "Epoch 00116 | Loss: 0.4248\n",
      "F1-Score: 0.5805\n",
      "Epoch 00117 | Loss: 0.4295\n",
      "Epoch 00118 | Loss: 0.4303\n",
      "Epoch 00119 | Loss: 0.4309\n",
      "Epoch 00120 | Loss: 0.4306\n",
      "Epoch 00121 | Loss: 0.4284\n",
      "F1-Score: 0.6129\n",
      "Epoch 00122 | Loss: 0.4282\n",
      "Epoch 00123 | Loss: 0.4264\n",
      "Epoch 00124 | Loss: 0.4237\n",
      "Epoch 00125 | Loss: 0.4236\n",
      "Epoch 00126 | Loss: 0.4235\n",
      "F1-Score: 0.6300\n",
      "Epoch 00127 | Loss: 0.4248\n",
      "Epoch 00128 | Loss: 0.4232\n",
      "Epoch 00129 | Loss: 0.4243\n",
      "Epoch 00130 | Loss: 0.4232\n",
      "Epoch 00131 | Loss: 0.4214\n",
      "F1-Score: 0.6084\n",
      "Epoch 00132 | Loss: 0.4195\n",
      "Epoch 00133 | Loss: 0.4184\n",
      "Epoch 00134 | Loss: 0.4172\n",
      "Epoch 00135 | Loss: 0.4156\n",
      "Epoch 00136 | Loss: 0.4139\n",
      "F1-Score: 0.6328\n",
      "Epoch 00137 | Loss: 0.4135\n",
      "Epoch 00138 | Loss: 0.4126\n",
      "Epoch 00139 | Loss: 0.4125\n",
      "Epoch 00140 | Loss: 0.4123\n",
      "Epoch 00141 | Loss: 0.4125\n",
      "F1-Score: 0.6074\n",
      "Epoch 00142 | Loss: 0.4118\n",
      "Epoch 00143 | Loss: 0.4116\n",
      "Epoch 00144 | Loss: 0.4128\n",
      "Epoch 00145 | Loss: 0.4144\n",
      "Epoch 00146 | Loss: 0.4160\n",
      "F1-Score: 0.6315\n",
      "Epoch 00147 | Loss: 0.4182\n",
      "Epoch 00148 | Loss: 0.4201\n",
      "Epoch 00149 | Loss: 0.4200\n",
      "Epoch 00150 | Loss: 0.4200\n",
      "Epoch 00151 | Loss: 0.4202\n",
      "F1-Score: 0.6231\n",
      "Epoch 00152 | Loss: 0.4187\n",
      "Epoch 00153 | Loss: 0.4163\n",
      "Epoch 00154 | Loss: 0.4161\n",
      "Epoch 00155 | Loss: 0.4160\n",
      "Epoch 00156 | Loss: 0.4144\n",
      "F1-Score: 0.6120\n",
      "Epoch 00157 | Loss: 0.4149\n",
      "Epoch 00158 | Loss: 0.4140\n",
      "Epoch 00159 | Loss: 0.4141\n",
      "Epoch 00160 | Loss: 0.4124\n",
      "Epoch 00161 | Loss: 0.4127\n",
      "F1-Score: 0.6227\n",
      "Epoch 00162 | Loss: 0.4119\n",
      "Epoch 00163 | Loss: 0.4120\n",
      "Epoch 00164 | Loss: 0.4106\n",
      "Epoch 00165 | Loss: 0.4106\n",
      "Epoch 00166 | Loss: 0.4097\n",
      "F1-Score: 0.5986\n",
      "Epoch 00167 | Loss: 0.4088\n",
      "Epoch 00168 | Loss: 0.4092\n",
      "Epoch 00169 | Loss: 0.4113\n",
      "Epoch 00170 | Loss: 0.4138\n",
      "Epoch 00171 | Loss: 0.4160\n",
      "F1-Score: 0.6551\n",
      "Epoch 00172 | Loss: 0.4160\n",
      "Epoch 00173 | Loss: 0.4217\n",
      "Epoch 00174 | Loss: 0.4216\n",
      "Epoch 00175 | Loss: 0.4169\n",
      "Epoch 00176 | Loss: 0.4137\n",
      "F1-Score: 0.6268\n",
      "Epoch 00177 | Loss: 0.4125\n",
      "Epoch 00178 | Loss: 0.4114\n",
      "Epoch 00179 | Loss: 0.4087\n",
      "Epoch 00180 | Loss: 0.4083\n",
      "Epoch 00181 | Loss: 0.4082\n",
      "F1-Score: 0.6219\n",
      "Epoch 00182 | Loss: 0.4056\n",
      "Epoch 00183 | Loss: 0.4060\n",
      "Epoch 00184 | Loss: 0.4065\n",
      "Epoch 00185 | Loss: 0.4055\n",
      "Epoch 00186 | Loss: 0.4063\n",
      "F1-Score: 0.6136\n",
      "Epoch 00187 | Loss: 0.4060\n",
      "Epoch 00188 | Loss: 0.4060\n",
      "Epoch 00189 | Loss: 0.4059\n",
      "Epoch 00190 | Loss: 0.4056\n",
      "Epoch 00191 | Loss: 0.4050\n",
      "F1-Score: 0.6396\n",
      "Epoch 00192 | Loss: 0.4047\n",
      "Epoch 00193 | Loss: 0.4053\n",
      "Epoch 00194 | Loss: 0.4028\n",
      "Epoch 00195 | Loss: 0.4020\n",
      "Epoch 00196 | Loss: 0.4019\n",
      "F1-Score: 0.6529\n",
      "Epoch 00197 | Loss: 0.4022\n",
      "Epoch 00198 | Loss: 0.4023\n",
      "Epoch 00199 | Loss: 0.4037\n",
      "Epoch 00200 | Loss: 0.4057\n"
     ]
    }
   ],
   "source": [
    "### DEVICE GPU OR CPU : will select GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"\\nDevice: \", device)\n",
    "\n",
    "### Max number of epochs\n",
    "max_epochs = 200\n",
    "\n",
    "### DEFINE THE MODEL\n",
    "basic_model = BasicGraphModel(  input_size = n_features, \n",
    "                                hidden_size = 256, \n",
    "                                output_size = n_classes).to(device)\n",
    "\n",
    "### DEFINE LOSS FUNCTION\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "### DEFINE OPTIMIZER\n",
    "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
    "\n",
    "### TRAIN THE MODEL\n",
    "epoch_list, basic_model_scores = train(basic_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sGc5g7Xmap2"
   },
   "source": [
    "Let's evaluate the performance of this basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "ztfbbg2TNP7F",
    "outputId": "a0b6afc0-a7e8-4afa-e90b-8909ce275a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Model : F1-Score on the test set: 0.6026\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### F1-SCORE ON TEST DATASET\n",
    "score_test = evaluate(basic_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Basic Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "### PLOT EVOLUTION OF F1-SCORE W.R.T EPOCHS\n",
    "def plot_f1_score(epoch_list, scores) :\n",
    "    plt.figure(figsize=[10,5])\n",
    "    plt.plot(epoch_list, scores)\n",
    "    plt.title(\"Evolution of F1S-Score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.show()\n",
    "    \n",
    "plot_f1_score(epoch_list, basic_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRVsy5vLnWm_"
   },
   "source": [
    "### Define a better model\n",
    "\n",
    "Now, it's your turn to improve this basic model ! To do so, complete whenever ###### YOUR ANSWER ######## and run the two following cells.\n",
    "\n",
    "**HINT :** https://arxiv.org/pdf/1710.10903.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTo7PxFpRHzL"
   },
   "outputs": [],
   "source": [
    "class StudentModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "      super().__init__()\n",
    "      self.GATconv_1 = graphnn.GATConv(input_size,hidden_size,heads=4)\n",
    "      self.GATconv_2 = graphnn.GATConv(hidden_size*4,hidden_size,heads=4)\n",
    "      self.GATconv_3 = graphnn.GATConv(hidden_size*4,output_size,heads=6,concat=False)\n",
    "      self.elu = nn.ELU()\n",
    "      \n",
    "  def forward(self, x, edge_index):\n",
    "    out_nodes= self.elu(self.GATconv_1(x,edge_index))\n",
    "    out_nodes = self.elu(self.GATconv_2(out_nodes,edge_index)+out_nodes)\n",
    "    out_nodes = self.GATconv_3(out_nodes,edge_index)\n",
    "\n",
    "    return out_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yGdQ2uxpCAX"
   },
   "source": [
    "Let's train your model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36jkT_bzNoWs"
   },
   "outputs": [],
   "source": [
    "def train_auto(model, loss_fcn, device, optimizer, train_dataloader, val_dataloader):\n",
    "\n",
    "    epoch_list = []\n",
    "    scores_list = []\n",
    "    best_F1 = 0\n",
    "    patience = 0\n",
    "    epoch=0\n",
    "    # while the score is increasing\n",
    "    while True :\n",
    "        patience +=1\n",
    "        model.train()\n",
    "        losses = []\n",
    "        # loop over batches\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_device = train_batch.to(device)\n",
    "            # logits is the output of the model\n",
    "            logits = model(train_batch_device.x, train_batch_device.edge_index)\n",
    "            # compute the loss\n",
    "            loss = loss_fcn(logits, train_batch_device.y)\n",
    "            # optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        loss_data = np.array(losses).mean()\n",
    "        print(\"Epoch {:05d} | Loss: {:.4f}\".format(epoch+1, loss_data))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            # evaluate the model on the validation set\n",
    "            # computes the f1-score (see next function)\n",
    "            score = evaluate(model, loss_fcn, device, val_dataloader)\n",
    "            if score>best_F1:\n",
    "              best_F1 = score\n",
    "              patience = 0\n",
    "\n",
    "            print(\"F1-Score: {:.4f}\".format(score))\n",
    "            scores_list.append(score)\n",
    "            epoch_list.append(epoch)\n",
    "        if patience>100:\n",
    "          break\n",
    "        epoch+=1\n",
    "    return epoch_list, scores_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XIYzkYRo3AQ",
    "outputId": "841ca0e2-1b87-4e92-8a54-1fc931f14a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Loss: 0.7227\n",
      "F1-Score: 0.4096\n",
      "Epoch 00002 | Loss: 0.5611\n",
      "Epoch 00003 | Loss: 0.5414\n",
      "Epoch 00004 | Loss: 0.5318\n",
      "Epoch 00005 | Loss: 0.5165\n",
      "Epoch 00006 | Loss: 0.5066\n",
      "F1-Score: 0.4944\n",
      "Epoch 00007 | Loss: 0.4955\n",
      "Epoch 00008 | Loss: 0.4887\n",
      "Epoch 00009 | Loss: 0.4799\n",
      "Epoch 00010 | Loss: 0.4735\n",
      "Epoch 00011 | Loss: 0.4665\n",
      "F1-Score: 0.5868\n",
      "Epoch 00012 | Loss: 0.4587\n",
      "Epoch 00013 | Loss: 0.4542\n",
      "Epoch 00014 | Loss: 0.4523\n",
      "Epoch 00015 | Loss: 0.4417\n",
      "Epoch 00016 | Loss: 0.4342\n",
      "F1-Score: 0.6359\n",
      "Epoch 00017 | Loss: 0.4290\n",
      "Epoch 00018 | Loss: 0.4257\n",
      "Epoch 00019 | Loss: 0.4185\n",
      "Epoch 00020 | Loss: 0.4096\n",
      "Epoch 00021 | Loss: 0.4009\n",
      "F1-Score: 0.6434\n",
      "Epoch 00022 | Loss: 0.3962\n",
      "Epoch 00023 | Loss: 0.3954\n",
      "Epoch 00024 | Loss: 0.3943\n",
      "Epoch 00025 | Loss: 0.3919\n",
      "Epoch 00026 | Loss: 0.3833\n",
      "F1-Score: 0.6940\n",
      "Epoch 00027 | Loss: 0.3735\n",
      "Epoch 00028 | Loss: 0.3655\n",
      "Epoch 00029 | Loss: 0.3558\n",
      "Epoch 00030 | Loss: 0.3524\n",
      "Epoch 00031 | Loss: 0.3567\n",
      "F1-Score: 0.7017\n",
      "Epoch 00032 | Loss: 0.3579\n",
      "Epoch 00033 | Loss: 0.3562\n",
      "Epoch 00034 | Loss: 0.3474\n",
      "Epoch 00035 | Loss: 0.3410\n",
      "Epoch 00036 | Loss: 0.3412\n",
      "F1-Score: 0.7175\n",
      "Epoch 00037 | Loss: 0.3416\n",
      "Epoch 00038 | Loss: 0.3303\n",
      "Epoch 00039 | Loss: 0.3150\n",
      "Epoch 00040 | Loss: 0.3026\n",
      "Epoch 00041 | Loss: 0.2949\n",
      "F1-Score: 0.7590\n",
      "Epoch 00042 | Loss: 0.2872\n",
      "Epoch 00043 | Loss: 0.2804\n",
      "Epoch 00044 | Loss: 0.2776\n",
      "Epoch 00045 | Loss: 0.2720\n",
      "Epoch 00046 | Loss: 0.2680\n",
      "F1-Score: 0.7766\n",
      "Epoch 00047 | Loss: 0.2620\n",
      "Epoch 00048 | Loss: 0.2571\n",
      "Epoch 00049 | Loss: 0.2536\n",
      "Epoch 00050 | Loss: 0.2526\n",
      "Epoch 00051 | Loss: 0.2497\n",
      "F1-Score: 0.7954\n",
      "Epoch 00052 | Loss: 0.2458\n",
      "Epoch 00053 | Loss: 0.2398\n",
      "Epoch 00054 | Loss: 0.2368\n",
      "Epoch 00055 | Loss: 0.2323\n",
      "Epoch 00056 | Loss: 0.2290\n",
      "F1-Score: 0.8003\n",
      "Epoch 00057 | Loss: 0.2281\n",
      "Epoch 00058 | Loss: 0.2282\n",
      "Epoch 00059 | Loss: 0.2260\n",
      "Epoch 00060 | Loss: 0.2257\n",
      "Epoch 00061 | Loss: 0.2289\n",
      "F1-Score: 0.7905\n",
      "Epoch 00062 | Loss: 0.2388\n",
      "Epoch 00063 | Loss: 0.2421\n",
      "Epoch 00064 | Loss: 0.2342\n",
      "Epoch 00065 | Loss: 0.2235\n",
      "Epoch 00066 | Loss: 0.2092\n",
      "F1-Score: 0.8236\n",
      "Epoch 00067 | Loss: 0.2003\n",
      "Epoch 00068 | Loss: 0.1958\n",
      "Epoch 00069 | Loss: 0.1903\n",
      "Epoch 00070 | Loss: 0.1831\n",
      "Epoch 00071 | Loss: 0.1789\n",
      "F1-Score: 0.8509\n",
      "Epoch 00072 | Loss: 0.1762\n",
      "Epoch 00073 | Loss: 0.1751\n",
      "Epoch 00074 | Loss: 0.1731\n",
      "Epoch 00075 | Loss: 0.1727\n",
      "Epoch 00076 | Loss: 0.1733\n",
      "F1-Score: 0.8427\n",
      "Epoch 00077 | Loss: 0.1813\n",
      "Epoch 00078 | Loss: 0.1834\n",
      "Epoch 00079 | Loss: 0.1833\n",
      "Epoch 00080 | Loss: 0.1772\n",
      "Epoch 00081 | Loss: 0.1727\n",
      "F1-Score: 0.8548\n",
      "Epoch 00082 | Loss: 0.1684\n",
      "Epoch 00083 | Loss: 0.1641\n",
      "Epoch 00084 | Loss: 0.1656\n",
      "Epoch 00085 | Loss: 0.1654\n",
      "Epoch 00086 | Loss: 0.1643\n",
      "F1-Score: 0.8520\n",
      "Epoch 00087 | Loss: 0.1575\n",
      "Epoch 00088 | Loss: 0.1563\n",
      "Epoch 00089 | Loss: 0.1508\n",
      "Epoch 00090 | Loss: 0.1477\n",
      "Epoch 00091 | Loss: 0.1446\n",
      "F1-Score: 0.8709\n",
      "Epoch 00092 | Loss: 0.1414\n",
      "Epoch 00093 | Loss: 0.1393\n",
      "Epoch 00094 | Loss: 0.1357\n",
      "Epoch 00095 | Loss: 0.1328\n",
      "Epoch 00096 | Loss: 0.1287\n",
      "F1-Score: 0.8812\n",
      "Epoch 00097 | Loss: 0.1265\n",
      "Epoch 00098 | Loss: 0.1256\n",
      "Epoch 00099 | Loss: 0.1307\n",
      "Epoch 00100 | Loss: 0.1327\n",
      "Epoch 00101 | Loss: 0.1334\n",
      "F1-Score: 0.8784\n",
      "Epoch 00102 | Loss: 0.1303\n",
      "Epoch 00103 | Loss: 0.1274\n",
      "Epoch 00104 | Loss: 0.1219\n",
      "Epoch 00105 | Loss: 0.1182\n",
      "Epoch 00106 | Loss: 0.1170\n",
      "F1-Score: 0.8903\n",
      "Epoch 00107 | Loss: 0.1189\n",
      "Epoch 00108 | Loss: 0.1194\n",
      "Epoch 00109 | Loss: 0.1198\n",
      "Epoch 00110 | Loss: 0.1188\n",
      "Epoch 00111 | Loss: 0.1188\n",
      "F1-Score: 0.8875\n",
      "Epoch 00112 | Loss: 0.1289\n",
      "Epoch 00113 | Loss: 0.1329\n",
      "Epoch 00114 | Loss: 0.1318\n",
      "Epoch 00115 | Loss: 0.1260\n",
      "Epoch 00116 | Loss: 0.1189\n",
      "F1-Score: 0.8880\n",
      "Epoch 00117 | Loss: 0.1142\n",
      "Epoch 00118 | Loss: 0.1085\n",
      "Epoch 00119 | Loss: 0.1029\n",
      "Epoch 00120 | Loss: 0.0999\n",
      "Epoch 00121 | Loss: 0.0972\n",
      "F1-Score: 0.8968\n",
      "Epoch 00122 | Loss: 0.0966\n",
      "Epoch 00123 | Loss: 0.0947\n",
      "Epoch 00124 | Loss: 0.0950\n",
      "Epoch 00125 | Loss: 0.0969\n",
      "Epoch 00126 | Loss: 0.0976\n",
      "F1-Score: 0.8962\n",
      "Epoch 00127 | Loss: 0.1016\n",
      "Epoch 00128 | Loss: 0.1012\n",
      "Epoch 00129 | Loss: 0.1001\n",
      "Epoch 00130 | Loss: 0.0983\n",
      "Epoch 00131 | Loss: 0.0985\n",
      "F1-Score: 0.8943\n",
      "Epoch 00132 | Loss: 0.0995\n",
      "Epoch 00133 | Loss: 0.0983\n",
      "Epoch 00134 | Loss: 0.0953\n",
      "Epoch 00135 | Loss: 0.0915\n",
      "Epoch 00136 | Loss: 0.0872\n",
      "F1-Score: 0.9027\n",
      "Epoch 00137 | Loss: 0.0851\n",
      "Epoch 00138 | Loss: 0.0818\n",
      "Epoch 00139 | Loss: 0.0814\n",
      "Epoch 00140 | Loss: 0.0845\n",
      "Epoch 00141 | Loss: 0.0899\n",
      "F1-Score: 0.8959\n",
      "Epoch 00142 | Loss: 0.0959\n",
      "Epoch 00143 | Loss: 0.1037\n",
      "Epoch 00144 | Loss: 0.1052\n",
      "Epoch 00145 | Loss: 0.1051\n",
      "Epoch 00146 | Loss: 0.1057\n",
      "F1-Score: 0.8992\n",
      "Epoch 00147 | Loss: 0.1019\n",
      "Epoch 00148 | Loss: 0.0933\n",
      "Epoch 00149 | Loss: 0.0891\n",
      "Epoch 00150 | Loss: 0.0858\n",
      "Epoch 00151 | Loss: 0.0797\n",
      "F1-Score: 0.9087\n",
      "Epoch 00152 | Loss: 0.0777\n",
      "Epoch 00153 | Loss: 0.0787\n",
      "Epoch 00154 | Loss: 0.0760\n",
      "Epoch 00155 | Loss: 0.0744\n",
      "Epoch 00156 | Loss: 0.0722\n",
      "F1-Score: 0.9114\n",
      "Epoch 00157 | Loss: 0.0718\n",
      "Epoch 00158 | Loss: 0.0734\n",
      "Epoch 00159 | Loss: 0.0731\n",
      "Epoch 00160 | Loss: 0.0726\n",
      "Epoch 00161 | Loss: 0.0728\n",
      "F1-Score: 0.9101\n",
      "Epoch 00162 | Loss: 0.0719\n",
      "Epoch 00163 | Loss: 0.0722\n",
      "Epoch 00164 | Loss: 0.0765\n",
      "Epoch 00165 | Loss: 0.0774\n",
      "Epoch 00166 | Loss: 0.0771\n",
      "F1-Score: 0.9077\n",
      "Epoch 00167 | Loss: 0.0753\n",
      "Epoch 00168 | Loss: 0.0750\n",
      "Epoch 00169 | Loss: 0.0735\n",
      "Epoch 00170 | Loss: 0.0706\n",
      "Epoch 00171 | Loss: 0.0730\n",
      "F1-Score: 0.9140\n",
      "Epoch 00172 | Loss: 0.0705\n",
      "Epoch 00173 | Loss: 0.0667\n",
      "Epoch 00174 | Loss: 0.0673\n",
      "Epoch 00175 | Loss: 0.0669\n",
      "Epoch 00176 | Loss: 0.0649\n",
      "F1-Score: 0.9154\n",
      "Epoch 00177 | Loss: 0.0645\n",
      "Epoch 00178 | Loss: 0.0663\n",
      "Epoch 00179 | Loss: 0.0642\n",
      "Epoch 00180 | Loss: 0.0633\n",
      "Epoch 00181 | Loss: 0.0626\n",
      "F1-Score: 0.9167\n",
      "Epoch 00182 | Loss: 0.0611\n",
      "Epoch 00183 | Loss: 0.0597\n",
      "Epoch 00184 | Loss: 0.0573\n",
      "Epoch 00185 | Loss: 0.0568\n",
      "Epoch 00186 | Loss: 0.0591\n",
      "F1-Score: 0.9194\n",
      "Epoch 00187 | Loss: 0.0608\n",
      "Epoch 00188 | Loss: 0.0593\n",
      "Epoch 00189 | Loss: 0.0615\n",
      "Epoch 00190 | Loss: 0.0638\n",
      "Epoch 00191 | Loss: 0.0625\n",
      "F1-Score: 0.9120\n",
      "Epoch 00192 | Loss: 0.0660\n",
      "Epoch 00193 | Loss: 0.0680\n",
      "Epoch 00194 | Loss: 0.0671\n",
      "Epoch 00195 | Loss: 0.0650\n",
      "Epoch 00196 | Loss: 0.0640\n",
      "F1-Score: 0.9169\n",
      "Epoch 00197 | Loss: 0.0629\n",
      "Epoch 00198 | Loss: 0.0616\n",
      "Epoch 00199 | Loss: 0.0597\n",
      "Epoch 00200 | Loss: 0.0594\n",
      "Epoch 00201 | Loss: 0.0590\n",
      "F1-Score: 0.9170\n",
      "Epoch 00202 | Loss: 0.0599\n",
      "Epoch 00203 | Loss: 0.0606\n",
      "Epoch 00204 | Loss: 0.0571\n",
      "Epoch 00205 | Loss: 0.0544\n",
      "Epoch 00206 | Loss: 0.0519\n",
      "F1-Score: 0.9234\n",
      "Epoch 00207 | Loss: 0.0511\n",
      "Epoch 00208 | Loss: 0.0503\n",
      "Epoch 00209 | Loss: 0.0508\n",
      "Epoch 00210 | Loss: 0.0496\n",
      "Epoch 00211 | Loss: 0.0494\n",
      "F1-Score: 0.9260\n",
      "Epoch 00212 | Loss: 0.0495\n",
      "Epoch 00213 | Loss: 0.0507\n",
      "Epoch 00214 | Loss: 0.0527\n",
      "Epoch 00215 | Loss: 0.0543\n",
      "Epoch 00216 | Loss: 0.0537\n",
      "F1-Score: 0.9213\n",
      "Epoch 00217 | Loss: 0.0547\n",
      "Epoch 00218 | Loss: 0.0559\n",
      "Epoch 00219 | Loss: 0.0594\n",
      "Epoch 00220 | Loss: 0.0624\n",
      "Epoch 00221 | Loss: 0.0637\n",
      "F1-Score: 0.9190\n",
      "Epoch 00222 | Loss: 0.0682\n",
      "Epoch 00223 | Loss: 0.0674\n",
      "Epoch 00224 | Loss: 0.0638\n",
      "Epoch 00225 | Loss: 0.0588\n",
      "Epoch 00226 | Loss: 0.0542\n",
      "F1-Score: 0.9222\n",
      "Epoch 00227 | Loss: 0.0505\n",
      "Epoch 00228 | Loss: 0.0473\n",
      "Epoch 00229 | Loss: 0.0421\n",
      "Epoch 00230 | Loss: 0.0393\n",
      "Epoch 00231 | Loss: 0.0393\n",
      "F1-Score: 0.9301\n",
      "Epoch 00232 | Loss: 0.0389\n",
      "Epoch 00233 | Loss: 0.0374\n",
      "Epoch 00234 | Loss: 0.0365\n",
      "Epoch 00235 | Loss: 0.0373\n",
      "Epoch 00236 | Loss: 0.0367\n",
      "F1-Score: 0.9319\n",
      "Epoch 00237 | Loss: 0.0371\n",
      "Epoch 00238 | Loss: 0.0383\n",
      "Epoch 00239 | Loss: 0.0410\n",
      "Epoch 00240 | Loss: 0.0429\n",
      "Epoch 00241 | Loss: 0.0427\n",
      "F1-Score: 0.9275\n",
      "Epoch 00242 | Loss: 0.0409\n",
      "Epoch 00243 | Loss: 0.0407\n",
      "Epoch 00244 | Loss: 0.0395\n",
      "Epoch 00245 | Loss: 0.0401\n",
      "Epoch 00246 | Loss: 0.0398\n",
      "F1-Score: 0.9264\n",
      "Epoch 00247 | Loss: 0.0396\n",
      "Epoch 00248 | Loss: 0.0393\n",
      "Epoch 00249 | Loss: 0.0393\n",
      "Epoch 00250 | Loss: 0.0373\n",
      "Epoch 00251 | Loss: 0.0394\n",
      "F1-Score: 0.9289\n",
      "Epoch 00252 | Loss: 0.0406\n",
      "Epoch 00253 | Loss: 0.0385\n",
      "Epoch 00254 | Loss: 0.0374\n",
      "Epoch 00255 | Loss: 0.0386\n",
      "Epoch 00256 | Loss: 0.0388\n",
      "F1-Score: 0.9289\n",
      "Epoch 00257 | Loss: 0.0395\n",
      "Epoch 00258 | Loss: 0.0409\n",
      "Epoch 00259 | Loss: 0.0449\n",
      "Epoch 00260 | Loss: 0.0497\n",
      "Epoch 00261 | Loss: 0.0622\n",
      "F1-Score: 0.9136\n",
      "Epoch 00262 | Loss: 0.0751\n",
      "Epoch 00263 | Loss: 0.0820\n",
      "Epoch 00264 | Loss: 0.0781\n",
      "Epoch 00265 | Loss: 0.0729\n",
      "Epoch 00266 | Loss: 0.0697\n",
      "F1-Score: 0.9165\n",
      "Epoch 00267 | Loss: 0.0658\n",
      "Epoch 00268 | Loss: 0.0586\n",
      "Epoch 00269 | Loss: 0.0545\n",
      "Epoch 00270 | Loss: 0.0512\n",
      "Epoch 00271 | Loss: 0.0417\n",
      "F1-Score: 0.9297\n",
      "Epoch 00272 | Loss: 0.0364\n",
      "Epoch 00273 | Loss: 0.0322\n",
      "Epoch 00274 | Loss: 0.0294\n",
      "Epoch 00275 | Loss: 0.0282\n",
      "Epoch 00276 | Loss: 0.0269\n",
      "F1-Score: 0.9374\n",
      "Epoch 00277 | Loss: 0.0274\n",
      "Epoch 00278 | Loss: 0.0283\n",
      "Epoch 00279 | Loss: 0.0289\n",
      "Epoch 00280 | Loss: 0.0290\n",
      "Epoch 00281 | Loss: 0.0291\n",
      "F1-Score: 0.9344\n",
      "Epoch 00282 | Loss: 0.0285\n",
      "Epoch 00283 | Loss: 0.0288\n",
      "Epoch 00284 | Loss: 0.0279\n",
      "Epoch 00285 | Loss: 0.0278\n",
      "Epoch 00286 | Loss: 0.0279\n",
      "F1-Score: 0.9333\n",
      "Epoch 00287 | Loss: 0.0280\n",
      "Epoch 00288 | Loss: 0.0270\n",
      "Epoch 00289 | Loss: 0.0269\n",
      "Epoch 00290 | Loss: 0.0267\n",
      "Epoch 00291 | Loss: 0.0270\n",
      "F1-Score: 0.9347\n",
      "Epoch 00292 | Loss: 0.0274\n",
      "Epoch 00293 | Loss: 0.0285\n",
      "Epoch 00294 | Loss: 0.0288\n",
      "Epoch 00295 | Loss: 0.0299\n",
      "Epoch 00296 | Loss: 0.0307\n",
      "F1-Score: 0.9330\n",
      "Epoch 00297 | Loss: 0.0310\n",
      "Epoch 00298 | Loss: 0.0321\n",
      "Epoch 00299 | Loss: 0.0308\n",
      "Epoch 00300 | Loss: 0.0313\n",
      "Epoch 00301 | Loss: 0.0318\n",
      "F1-Score: 0.9331\n",
      "Epoch 00302 | Loss: 0.0310\n",
      "Epoch 00303 | Loss: 0.0336\n",
      "Epoch 00304 | Loss: 0.0346\n",
      "Epoch 00305 | Loss: 0.0363\n",
      "Epoch 00306 | Loss: 0.0377\n",
      "F1-Score: 0.9315\n",
      "Epoch 00307 | Loss: 0.0380\n",
      "Epoch 00308 | Loss: 0.0402\n",
      "Epoch 00309 | Loss: 0.0409\n",
      "Epoch 00310 | Loss: 0.0403\n",
      "Epoch 00311 | Loss: 0.0388\n",
      "F1-Score: 0.9288\n",
      "Epoch 00312 | Loss: 0.0380\n",
      "Epoch 00313 | Loss: 0.0364\n",
      "Epoch 00314 | Loss: 0.0333\n",
      "Epoch 00315 | Loss: 0.0311\n",
      "Epoch 00316 | Loss: 0.0294\n",
      "F1-Score: 0.9344\n",
      "Epoch 00317 | Loss: 0.0280\n",
      "Epoch 00318 | Loss: 0.0269\n",
      "Epoch 00319 | Loss: 0.0255\n",
      "Epoch 00320 | Loss: 0.0240\n",
      "Epoch 00321 | Loss: 0.0232\n",
      "F1-Score: 0.9380\n",
      "Epoch 00322 | Loss: 0.0221\n",
      "Epoch 00323 | Loss: 0.0212\n",
      "Epoch 00324 | Loss: 0.0215\n",
      "Epoch 00325 | Loss: 0.0225\n",
      "Epoch 00326 | Loss: 0.0232\n",
      "F1-Score: 0.9373\n",
      "Epoch 00327 | Loss: 0.0232\n",
      "Epoch 00328 | Loss: 0.0258\n",
      "Epoch 00329 | Loss: 0.0276\n",
      "Epoch 00330 | Loss: 0.0280\n",
      "Epoch 00331 | Loss: 0.0284\n",
      "F1-Score: 0.9352\n",
      "Epoch 00332 | Loss: 0.0295\n",
      "Epoch 00333 | Loss: 0.0294\n",
      "Epoch 00334 | Loss: 0.0296\n",
      "Epoch 00335 | Loss: 0.0306\n",
      "Epoch 00336 | Loss: 0.0315\n",
      "F1-Score: 0.9329\n",
      "Epoch 00337 | Loss: 0.0319\n",
      "Epoch 00338 | Loss: 0.0306\n",
      "Epoch 00339 | Loss: 0.0315\n",
      "Epoch 00340 | Loss: 0.0365\n",
      "Epoch 00341 | Loss: 0.0504\n",
      "F1-Score: 0.9205\n",
      "Epoch 00342 | Loss: 0.0540\n",
      "Epoch 00343 | Loss: 0.0617\n",
      "Epoch 00344 | Loss: 0.0816\n",
      "Epoch 00345 | Loss: 0.0859\n",
      "Epoch 00346 | Loss: 0.0844\n",
      "F1-Score: 0.9079\n",
      "Epoch 00347 | Loss: 0.0797\n",
      "Epoch 00348 | Loss: 0.0782\n",
      "Epoch 00349 | Loss: 0.0713\n",
      "Epoch 00350 | Loss: 0.0627\n",
      "Epoch 00351 | Loss: 0.0646\n",
      "F1-Score: 0.9151\n",
      "Epoch 00352 | Loss: 0.0620\n",
      "Epoch 00353 | Loss: 0.0524\n",
      "Epoch 00354 | Loss: 0.0456\n",
      "Epoch 00355 | Loss: 0.0384\n",
      "Epoch 00356 | Loss: 0.0310\n",
      "F1-Score: 0.9360\n",
      "Epoch 00357 | Loss: 0.0267\n",
      "Epoch 00358 | Loss: 0.0280\n",
      "Epoch 00359 | Loss: 0.0361\n",
      "Epoch 00360 | Loss: 0.0310\n",
      "Epoch 00361 | Loss: 0.0259\n",
      "F1-Score: 0.9383\n",
      "Epoch 00362 | Loss: 0.0225\n",
      "Epoch 00363 | Loss: 0.0209\n",
      "Epoch 00364 | Loss: 0.0208\n",
      "Epoch 00365 | Loss: 0.0213\n",
      "Epoch 00366 | Loss: 0.0197\n",
      "F1-Score: 0.9399\n",
      "Epoch 00367 | Loss: 0.0188\n",
      "Epoch 00368 | Loss: 0.0182\n",
      "Epoch 00369 | Loss: 0.0190\n",
      "Epoch 00370 | Loss: 0.0186\n",
      "Epoch 00371 | Loss: 0.0190\n",
      "F1-Score: 0.9378\n",
      "Epoch 00372 | Loss: 0.0189\n",
      "Epoch 00373 | Loss: 0.0196\n",
      "Epoch 00374 | Loss: 0.0193\n",
      "Epoch 00375 | Loss: 0.0199\n",
      "Epoch 00376 | Loss: 0.0202\n",
      "F1-Score: 0.9388\n",
      "Epoch 00377 | Loss: 0.0207\n",
      "Epoch 00378 | Loss: 0.0199\n",
      "Epoch 00379 | Loss: 0.0193\n",
      "Epoch 00380 | Loss: 0.0188\n",
      "Epoch 00381 | Loss: 0.0185\n",
      "F1-Score: 0.9410\n",
      "Epoch 00382 | Loss: 0.0180\n",
      "Epoch 00383 | Loss: 0.0202\n",
      "Epoch 00384 | Loss: 0.0195\n",
      "Epoch 00385 | Loss: 0.0194\n",
      "Epoch 00386 | Loss: 0.0201\n",
      "F1-Score: 0.9386\n",
      "Epoch 00387 | Loss: 0.0201\n",
      "Epoch 00388 | Loss: 0.0206\n",
      "Epoch 00389 | Loss: 0.0220\n",
      "Epoch 00390 | Loss: 0.0216\n",
      "Epoch 00391 | Loss: 0.0225\n",
      "F1-Score: 0.9350\n",
      "Epoch 00392 | Loss: 0.0301\n",
      "Epoch 00393 | Loss: 0.0311\n",
      "Epoch 00394 | Loss: 0.0333\n",
      "Epoch 00395 | Loss: 0.0337\n",
      "Epoch 00396 | Loss: 0.0358\n",
      "F1-Score: 0.9300\n",
      "Epoch 00397 | Loss: 0.0360\n",
      "Epoch 00398 | Loss: 0.0417\n",
      "Epoch 00399 | Loss: 0.0492\n",
      "Epoch 00400 | Loss: 0.0530\n",
      "Epoch 00401 | Loss: 0.0567\n",
      "F1-Score: 0.9217\n",
      "Epoch 00402 | Loss: 0.0750\n",
      "Epoch 00403 | Loss: 0.0767\n",
      "Epoch 00404 | Loss: 0.0742\n",
      "Epoch 00405 | Loss: 0.0704\n",
      "Epoch 00406 | Loss: 0.0652\n",
      "F1-Score: 0.9241\n",
      "Epoch 00407 | Loss: 0.0572\n",
      "Epoch 00408 | Loss: 0.0474\n",
      "Epoch 00409 | Loss: 0.0384\n",
      "Epoch 00410 | Loss: 0.0300\n",
      "Epoch 00411 | Loss: 0.0247\n",
      "F1-Score: 0.9411\n",
      "Epoch 00412 | Loss: 0.0219\n",
      "Epoch 00413 | Loss: 0.0192\n",
      "Epoch 00414 | Loss: 0.0177\n",
      "Epoch 00415 | Loss: 0.0171\n",
      "Epoch 00416 | Loss: 0.0164\n",
      "F1-Score: 0.9418\n",
      "Epoch 00417 | Loss: 0.0161\n",
      "Epoch 00418 | Loss: 0.0148\n",
      "Epoch 00419 | Loss: 0.0145\n",
      "Epoch 00420 | Loss: 0.0135\n",
      "Epoch 00421 | Loss: 0.0135\n",
      "F1-Score: 0.9463\n",
      "Epoch 00422 | Loss: 0.0139\n",
      "Epoch 00423 | Loss: 0.0139\n",
      "Epoch 00424 | Loss: 0.0141\n",
      "Epoch 00425 | Loss: 0.0143\n",
      "Epoch 00426 | Loss: 0.0146\n",
      "F1-Score: 0.9432\n",
      "Epoch 00427 | Loss: 0.0149\n",
      "Epoch 00428 | Loss: 0.0153\n",
      "Epoch 00429 | Loss: 0.0151\n",
      "Epoch 00430 | Loss: 0.0148\n",
      "Epoch 00431 | Loss: 0.0151\n",
      "F1-Score: 0.9423\n",
      "Epoch 00432 | Loss: 0.0153\n",
      "Epoch 00433 | Loss: 0.0142\n",
      "Epoch 00434 | Loss: 0.0147\n",
      "Epoch 00435 | Loss: 0.0147\n",
      "Epoch 00436 | Loss: 0.0161\n",
      "F1-Score: 0.9422\n",
      "Epoch 00437 | Loss: 0.0168\n",
      "Epoch 00438 | Loss: 0.0171\n",
      "Epoch 00439 | Loss: 0.0197\n",
      "Epoch 00440 | Loss: 0.0198\n",
      "Epoch 00441 | Loss: 0.0199\n",
      "F1-Score: 0.9384\n",
      "Epoch 00442 | Loss: 0.0203\n",
      "Epoch 00443 | Loss: 0.0209\n",
      "Epoch 00444 | Loss: 0.0193\n",
      "Epoch 00445 | Loss: 0.0184\n",
      "Epoch 00446 | Loss: 0.0172\n",
      "F1-Score: 0.9417\n",
      "Epoch 00447 | Loss: 0.0169\n",
      "Epoch 00448 | Loss: 0.0162\n",
      "Epoch 00449 | Loss: 0.0163\n",
      "Epoch 00450 | Loss: 0.0159\n",
      "Epoch 00451 | Loss: 0.0170\n",
      "F1-Score: 0.9412\n",
      "Epoch 00452 | Loss: 0.0185\n",
      "Epoch 00453 | Loss: 0.0181\n",
      "Epoch 00454 | Loss: 0.0188\n",
      "Epoch 00455 | Loss: 0.0187\n",
      "Epoch 00456 | Loss: 0.0194\n",
      "F1-Score: 0.9394\n",
      "Epoch 00457 | Loss: 0.0196\n",
      "Epoch 00458 | Loss: 0.0204\n",
      "Epoch 00459 | Loss: 0.0217\n",
      "Epoch 00460 | Loss: 0.0226\n",
      "Epoch 00461 | Loss: 0.0221\n",
      "F1-Score: 0.9398\n",
      "Epoch 00462 | Loss: 0.0250\n",
      "Epoch 00463 | Loss: 0.0269\n",
      "Epoch 00464 | Loss: 0.0288\n",
      "Epoch 00465 | Loss: 0.0322\n",
      "Epoch 00466 | Loss: 0.0453\n",
      "F1-Score: 0.9255\n",
      "Epoch 00467 | Loss: 0.0486\n",
      "Epoch 00468 | Loss: 0.0574\n",
      "Epoch 00469 | Loss: 0.0697\n",
      "Epoch 00470 | Loss: 0.0902\n",
      "Epoch 00471 | Loss: 0.0955\n",
      "F1-Score: 0.9125\n",
      "Epoch 00472 | Loss: 0.0808\n",
      "Epoch 00473 | Loss: 0.0763\n",
      "Epoch 00474 | Loss: 0.0697\n",
      "Epoch 00475 | Loss: 0.0597\n",
      "Epoch 00476 | Loss: 0.0499\n",
      "F1-Score: 0.9284\n",
      "Epoch 00477 | Loss: 0.0392\n",
      "Epoch 00478 | Loss: 0.0323\n",
      "Epoch 00479 | Loss: 0.0271\n",
      "Epoch 00480 | Loss: 0.0233\n",
      "Epoch 00481 | Loss: 0.0197\n",
      "F1-Score: 0.9445\n",
      "Epoch 00482 | Loss: 0.0166\n",
      "Epoch 00483 | Loss: 0.0145\n",
      "Epoch 00484 | Loss: 0.0129\n",
      "Epoch 00485 | Loss: 0.0120\n",
      "Epoch 00486 | Loss: 0.0118\n",
      "F1-Score: 0.9489\n",
      "Epoch 00487 | Loss: 0.0115\n",
      "Epoch 00488 | Loss: 0.0121\n",
      "Epoch 00489 | Loss: 0.0124\n",
      "Epoch 00490 | Loss: 0.0128\n",
      "Epoch 00491 | Loss: 0.0127\n",
      "F1-Score: 0.9446\n",
      "Epoch 00492 | Loss: 0.0136\n",
      "Epoch 00493 | Loss: 0.0135\n",
      "Epoch 00494 | Loss: 0.0137\n",
      "Epoch 00495 | Loss: 0.0136\n",
      "Epoch 00496 | Loss: 0.0143\n",
      "F1-Score: 0.9451\n",
      "Epoch 00497 | Loss: 0.0140\n",
      "Epoch 00498 | Loss: 0.0140\n",
      "Epoch 00499 | Loss: 0.0131\n",
      "Epoch 00500 | Loss: 0.0122\n",
      "Epoch 00501 | Loss: 0.0115\n",
      "F1-Score: 0.9470\n",
      "Epoch 00502 | Loss: 0.0105\n",
      "Epoch 00503 | Loss: 0.0098\n",
      "Epoch 00504 | Loss: 0.0090\n",
      "Epoch 00505 | Loss: 0.0089\n",
      "Epoch 00506 | Loss: 0.0090\n",
      "F1-Score: 0.9486\n",
      "Epoch 00507 | Loss: 0.0087\n",
      "Epoch 00508 | Loss: 0.0087\n",
      "Epoch 00509 | Loss: 0.0090\n",
      "Epoch 00510 | Loss: 0.0094\n",
      "Epoch 00511 | Loss: 0.0092\n",
      "F1-Score: 0.9465\n",
      "Epoch 00512 | Loss: 0.0096\n",
      "Epoch 00513 | Loss: 0.0102\n",
      "Epoch 00514 | Loss: 0.0110\n",
      "Epoch 00515 | Loss: 0.0116\n",
      "Epoch 00516 | Loss: 0.0126\n",
      "F1-Score: 0.9440\n",
      "Epoch 00517 | Loss: 0.0144\n",
      "Epoch 00518 | Loss: 0.0164\n",
      "Epoch 00519 | Loss: 0.0198\n",
      "Epoch 00520 | Loss: 0.0213\n",
      "Epoch 00521 | Loss: 0.0224\n",
      "F1-Score: 0.9397\n",
      "Epoch 00522 | Loss: 0.0236\n",
      "Epoch 00523 | Loss: 0.0293\n",
      "Epoch 00524 | Loss: 0.0294\n",
      "Epoch 00525 | Loss: 0.0286\n",
      "Epoch 00526 | Loss: 0.0305\n",
      "F1-Score: 0.9324\n",
      "Epoch 00527 | Loss: 0.0340\n",
      "Epoch 00528 | Loss: 0.0365\n",
      "Epoch 00529 | Loss: 0.0405\n",
      "Epoch 00530 | Loss: 0.0544\n",
      "Epoch 00531 | Loss: 0.0695\n",
      "F1-Score: 0.9159\n",
      "Epoch 00532 | Loss: 0.0833\n",
      "Epoch 00533 | Loss: 0.0801\n",
      "Epoch 00534 | Loss: 0.0771\n",
      "Epoch 00535 | Loss: 0.0672\n",
      "Epoch 00536 | Loss: 0.0600\n",
      "F1-Score: 0.9257\n",
      "Epoch 00537 | Loss: 0.0528\n",
      "Epoch 00538 | Loss: 0.0413\n",
      "Epoch 00539 | Loss: 0.0335\n",
      "Epoch 00540 | Loss: 0.0260\n",
      "Epoch 00541 | Loss: 0.0216\n",
      "F1-Score: 0.9431\n",
      "Epoch 00542 | Loss: 0.0181\n",
      "Epoch 00543 | Loss: 0.0157\n",
      "Epoch 00544 | Loss: 0.0146\n",
      "Epoch 00545 | Loss: 0.0142\n",
      "Epoch 00546 | Loss: 0.0131\n",
      "F1-Score: 0.9477\n",
      "Epoch 00547 | Loss: 0.0122\n",
      "Epoch 00548 | Loss: 0.0119\n",
      "Epoch 00549 | Loss: 0.0109\n",
      "Epoch 00550 | Loss: 0.0106\n",
      "Epoch 00551 | Loss: 0.0100\n",
      "F1-Score: 0.9496\n",
      "Epoch 00552 | Loss: 0.0100\n",
      "Epoch 00553 | Loss: 0.0100\n",
      "Epoch 00554 | Loss: 0.0097\n",
      "Epoch 00555 | Loss: 0.0094\n",
      "Epoch 00556 | Loss: 0.0098\n",
      "F1-Score: 0.9500\n",
      "Epoch 00557 | Loss: 0.0094\n",
      "Epoch 00558 | Loss: 0.0095\n",
      "Epoch 00559 | Loss: 0.0089\n",
      "Epoch 00560 | Loss: 0.0087\n",
      "Epoch 00561 | Loss: 0.0086\n",
      "F1-Score: 0.9503\n",
      "Epoch 00562 | Loss: 0.0088\n",
      "Epoch 00563 | Loss: 0.0082\n",
      "Epoch 00564 | Loss: 0.0080\n",
      "Epoch 00565 | Loss: 0.0083\n",
      "Epoch 00566 | Loss: 0.0086\n",
      "F1-Score: 0.9488\n",
      "Epoch 00567 | Loss: 0.0085\n",
      "Epoch 00568 | Loss: 0.0086\n",
      "Epoch 00569 | Loss: 0.0083\n",
      "Epoch 00570 | Loss: 0.0081\n",
      "Epoch 00571 | Loss: 0.0084\n",
      "F1-Score: 0.9505\n",
      "Epoch 00572 | Loss: 0.0086\n",
      "Epoch 00573 | Loss: 0.0084\n",
      "Epoch 00574 | Loss: 0.0085\n",
      "Epoch 00575 | Loss: 0.0086\n",
      "Epoch 00576 | Loss: 0.0090\n",
      "F1-Score: 0.9486\n",
      "Epoch 00577 | Loss: 0.0093\n",
      "Epoch 00578 | Loss: 0.0096\n",
      "Epoch 00579 | Loss: 0.0098\n",
      "Epoch 00580 | Loss: 0.0102\n",
      "Epoch 00581 | Loss: 0.0103\n",
      "F1-Score: 0.9473\n",
      "Epoch 00582 | Loss: 0.0105\n",
      "Epoch 00583 | Loss: 0.0109\n",
      "Epoch 00584 | Loss: 0.0105\n",
      "Epoch 00585 | Loss: 0.0105\n",
      "Epoch 00586 | Loss: 0.0106\n",
      "F1-Score: 0.9454\n",
      "Epoch 00587 | Loss: 0.0116\n",
      "Epoch 00588 | Loss: 0.0130\n",
      "Epoch 00589 | Loss: 0.0153\n",
      "Epoch 00590 | Loss: 0.0164\n",
      "Epoch 00591 | Loss: 0.0173\n",
      "F1-Score: 0.9384\n",
      "Epoch 00592 | Loss: 0.0303\n",
      "Epoch 00593 | Loss: 0.0374\n",
      "Epoch 00594 | Loss: 0.0468\n",
      "Epoch 00595 | Loss: 0.0792\n",
      "Epoch 00596 | Loss: 0.0937\n",
      "F1-Score: 0.9122\n",
      "Epoch 00597 | Loss: 0.0990\n",
      "Epoch 00598 | Loss: 0.0920\n",
      "Epoch 00599 | Loss: 0.0860\n",
      "Epoch 00600 | Loss: 0.0831\n",
      "Epoch 00601 | Loss: 0.0689\n",
      "F1-Score: 0.9210\n",
      "Epoch 00602 | Loss: 0.0573\n",
      "Epoch 00603 | Loss: 0.0473\n",
      "Epoch 00604 | Loss: 0.0368\n",
      "Epoch 00605 | Loss: 0.0322\n",
      "Epoch 00606 | Loss: 0.0268\n",
      "F1-Score: 0.9400\n",
      "Epoch 00607 | Loss: 0.0215\n",
      "Epoch 00608 | Loss: 0.0179\n",
      "Epoch 00609 | Loss: 0.0149\n",
      "Epoch 00610 | Loss: 0.0129\n",
      "Epoch 00611 | Loss: 0.0115\n",
      "F1-Score: 0.9498\n",
      "Epoch 00612 | Loss: 0.0106\n",
      "Epoch 00613 | Loss: 0.0099\n",
      "Epoch 00614 | Loss: 0.0094\n",
      "Epoch 00615 | Loss: 0.0089\n",
      "Epoch 00616 | Loss: 0.0087\n",
      "F1-Score: 0.9517\n",
      "Epoch 00617 | Loss: 0.0089\n",
      "Epoch 00618 | Loss: 0.0087\n",
      "Epoch 00619 | Loss: 0.0090\n",
      "Epoch 00620 | Loss: 0.0090\n",
      "Epoch 00621 | Loss: 0.0094\n",
      "F1-Score: 0.9517\n",
      "Epoch 00622 | Loss: 0.0092\n",
      "Epoch 00623 | Loss: 0.0096\n",
      "Epoch 00624 | Loss: 0.0098\n",
      "Epoch 00625 | Loss: 0.0098\n",
      "Epoch 00626 | Loss: 0.0097\n",
      "F1-Score: 0.9482\n",
      "Epoch 00627 | Loss: 0.0096\n",
      "Epoch 00628 | Loss: 0.0091\n",
      "Epoch 00629 | Loss: 0.0087\n",
      "Epoch 00630 | Loss: 0.0081\n",
      "Epoch 00631 | Loss: 0.0074\n",
      "F1-Score: 0.9518\n",
      "Epoch 00632 | Loss: 0.0070\n",
      "Epoch 00633 | Loss: 0.0071\n",
      "Epoch 00634 | Loss: 0.0064\n",
      "Epoch 00635 | Loss: 0.0064\n",
      "Epoch 00636 | Loss: 0.0059\n",
      "F1-Score: 0.9529\n",
      "Epoch 00637 | Loss: 0.0060\n",
      "Epoch 00638 | Loss: 0.0061\n",
      "Epoch 00639 | Loss: 0.0062\n",
      "Epoch 00640 | Loss: 0.0058\n",
      "Epoch 00641 | Loss: 0.0058\n",
      "F1-Score: 0.9532\n",
      "Epoch 00642 | Loss: 0.0057\n",
      "Epoch 00643 | Loss: 0.0058\n",
      "Epoch 00644 | Loss: 0.0058\n",
      "Epoch 00645 | Loss: 0.0056\n",
      "Epoch 00646 | Loss: 0.0056\n",
      "F1-Score: 0.9535\n",
      "Epoch 00647 | Loss: 0.0056\n",
      "Epoch 00648 | Loss: 0.0055\n",
      "Epoch 00649 | Loss: 0.0054\n",
      "Epoch 00650 | Loss: 0.0053\n",
      "Epoch 00651 | Loss: 0.0055\n",
      "F1-Score: 0.9528\n",
      "Epoch 00652 | Loss: 0.0054\n",
      "Epoch 00653 | Loss: 0.0054\n",
      "Epoch 00654 | Loss: 0.0056\n",
      "Epoch 00655 | Loss: 0.0058\n",
      "Epoch 00656 | Loss: 0.0055\n",
      "F1-Score: 0.9531\n",
      "Epoch 00657 | Loss: 0.0057\n",
      "Epoch 00658 | Loss: 0.0056\n",
      "Epoch 00659 | Loss: 0.0059\n",
      "Epoch 00660 | Loss: 0.0058\n",
      "Epoch 00661 | Loss: 0.0059\n",
      "F1-Score: 0.9526\n",
      "Epoch 00662 | Loss: 0.0054\n",
      "Epoch 00663 | Loss: 0.0054\n",
      "Epoch 00664 | Loss: 0.0056\n",
      "Epoch 00665 | Loss: 0.0056\n",
      "Epoch 00666 | Loss: 0.0055\n",
      "F1-Score: 0.9526\n",
      "Epoch 00667 | Loss: 0.0055\n",
      "Epoch 00668 | Loss: 0.0057\n",
      "Epoch 00669 | Loss: 0.0057\n",
      "Epoch 00670 | Loss: 0.0058\n",
      "Epoch 00671 | Loss: 0.0057\n",
      "F1-Score: 0.9523\n",
      "Epoch 00672 | Loss: 0.0058\n",
      "Epoch 00673 | Loss: 0.0055\n",
      "Epoch 00674 | Loss: 0.0054\n",
      "Epoch 00675 | Loss: 0.0053\n",
      "Epoch 00676 | Loss: 0.0054\n",
      "F1-Score: 0.9526\n",
      "Epoch 00677 | Loss: 0.0054\n",
      "Epoch 00678 | Loss: 0.0054\n",
      "Epoch 00679 | Loss: 0.0055\n",
      "Epoch 00680 | Loss: 0.0055\n",
      "Epoch 00681 | Loss: 0.0058\n",
      "F1-Score: 0.9525\n",
      "Epoch 00682 | Loss: 0.0054\n",
      "Epoch 00683 | Loss: 0.0052\n",
      "Epoch 00684 | Loss: 0.0054\n",
      "Epoch 00685 | Loss: 0.0054\n",
      "Epoch 00686 | Loss: 0.0053\n",
      "F1-Score: 0.9522\n",
      "Epoch 00687 | Loss: 0.0053\n",
      "Epoch 00688 | Loss: 0.0054\n",
      "Epoch 00689 | Loss: 0.0053\n",
      "Epoch 00690 | Loss: 0.0055\n",
      "Epoch 00691 | Loss: 0.0055\n",
      "F1-Score: 0.9518\n",
      "Epoch 00692 | Loss: 0.0057\n",
      "Epoch 00693 | Loss: 0.0057\n",
      "Epoch 00694 | Loss: 0.0059\n",
      "Epoch 00695 | Loss: 0.0054\n",
      "Epoch 00696 | Loss: 0.0054\n",
      "F1-Score: 0.9518\n",
      "Epoch 00697 | Loss: 0.0056\n",
      "Epoch 00698 | Loss: 0.0057\n",
      "Epoch 00699 | Loss: 0.0055\n",
      "Epoch 00700 | Loss: 0.0063\n",
      "Epoch 00701 | Loss: 0.0068\n",
      "F1-Score: 0.9497\n",
      "Epoch 00702 | Loss: 0.0103\n",
      "Epoch 00703 | Loss: 0.0158\n",
      "Epoch 00704 | Loss: 0.0321\n",
      "Epoch 00705 | Loss: 0.0818\n",
      "Epoch 00706 | Loss: 0.1650\n",
      "F1-Score: 0.8782\n",
      "Epoch 00707 | Loss: 0.2413\n",
      "Epoch 00708 | Loss: 0.2387\n",
      "Epoch 00709 | Loss: 0.1900\n",
      "Epoch 00710 | Loss: 0.1291\n",
      "Epoch 00711 | Loss: 0.0900\n",
      "F1-Score: 0.9210\n",
      "Epoch 00712 | Loss: 0.0627\n",
      "Epoch 00713 | Loss: 0.0482\n",
      "Epoch 00714 | Loss: 0.0394\n",
      "Epoch 00715 | Loss: 0.0317\n",
      "Epoch 00716 | Loss: 0.0252\n",
      "F1-Score: 0.9431\n",
      "Epoch 00717 | Loss: 0.0201\n",
      "Epoch 00718 | Loss: 0.0173\n",
      "Epoch 00719 | Loss: 0.0150\n",
      "Epoch 00720 | Loss: 0.0135\n",
      "Epoch 00721 | Loss: 0.0124\n",
      "F1-Score: 0.9501\n",
      "Epoch 00722 | Loss: 0.0120\n",
      "Epoch 00723 | Loss: 0.0114\n",
      "Epoch 00724 | Loss: 0.0108\n",
      "Epoch 00725 | Loss: 0.0108\n",
      "Epoch 00726 | Loss: 0.0111\n",
      "F1-Score: 0.9501\n",
      "Epoch 00727 | Loss: 0.0122\n",
      "Epoch 00728 | Loss: 0.0129\n",
      "Epoch 00729 | Loss: 0.0144\n",
      "Epoch 00730 | Loss: 0.0145\n",
      "Epoch 00731 | Loss: 0.0156\n",
      "F1-Score: 0.9442\n",
      "Epoch 00732 | Loss: 0.0154\n",
      "Epoch 00733 | Loss: 0.0152\n",
      "Epoch 00734 | Loss: 0.0143\n",
      "Epoch 00735 | Loss: 0.0139\n",
      "Epoch 00736 | Loss: 0.0128\n",
      "F1-Score: 0.9480\n",
      "Epoch 00737 | Loss: 0.0130\n",
      "Epoch 00738 | Loss: 0.0129\n",
      "Epoch 00739 | Loss: 0.0136\n",
      "Epoch 00740 | Loss: 0.0126\n",
      "Epoch 00741 | Loss: 0.0122\n",
      "F1-Score: 0.9481\n",
      "Epoch 00742 | Loss: 0.0115\n",
      "Epoch 00743 | Loss: 0.0107\n",
      "Epoch 00744 | Loss: 0.0102\n",
      "Epoch 00745 | Loss: 0.0093\n",
      "Epoch 00746 | Loss: 0.0086\n",
      "F1-Score: 0.9516\n",
      "Epoch 00747 | Loss: 0.0078\n"
     ]
    }
   ],
   "source": [
    "## Student model\n",
    "student_model = StudentModel(n_features,256,n_classes).to(device)\n",
    "\n",
    "### DEFINE LOSS FUNCTION AND OPTIMIZER\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=0.005)\n",
    "\n",
    "### TRAIN\n",
    "epoch_list, student_model_scores = train_auto(student_model, loss_fcn, device, optimizer, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37JCuJUrOw2E",
    "outputId": "9ca5c162-84d6-44cb-87e5-9c31b4b87718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Loss: 0.6386\n",
      "F1-Score: 0.4408\n",
      "Epoch 00002 | Loss: 0.5838\n",
      "Epoch 00003 | Loss: 0.5627\n",
      "Epoch 00004 | Loss: 0.5565\n",
      "Epoch 00005 | Loss: 0.5512\n",
      "Epoch 00006 | Loss: 0.5459\n",
      "F1-Score: 0.4938\n",
      "Epoch 00007 | Loss: 0.5419\n",
      "Epoch 00008 | Loss: 0.5376\n",
      "Epoch 00009 | Loss: 0.5341\n",
      "Epoch 00010 | Loss: 0.5316\n",
      "Epoch 00011 | Loss: 0.5288\n",
      "F1-Score: 0.5241\n",
      "Epoch 00012 | Loss: 0.5261\n",
      "Epoch 00013 | Loss: 0.5236\n",
      "Epoch 00014 | Loss: 0.5212\n",
      "Epoch 00015 | Loss: 0.5189\n",
      "Epoch 00016 | Loss: 0.5167\n",
      "F1-Score: 0.5364\n",
      "Epoch 00017 | Loss: 0.5145\n",
      "Epoch 00018 | Loss: 0.5124\n",
      "Epoch 00019 | Loss: 0.5104\n",
      "Epoch 00020 | Loss: 0.5085\n",
      "Epoch 00021 | Loss: 0.5068\n",
      "F1-Score: 0.5346\n",
      "Epoch 00022 | Loss: 0.5053\n",
      "Epoch 00023 | Loss: 0.5040\n",
      "Epoch 00024 | Loss: 0.5029\n",
      "Epoch 00025 | Loss: 0.5004\n",
      "Epoch 00026 | Loss: 0.4984\n",
      "F1-Score: 0.5306\n",
      "Epoch 00027 | Loss: 0.4966\n",
      "Epoch 00028 | Loss: 0.4944\n",
      "Epoch 00029 | Loss: 0.4922\n",
      "Epoch 00030 | Loss: 0.4900\n",
      "Epoch 00031 | Loss: 0.4878\n",
      "F1-Score: 0.5367\n",
      "Epoch 00032 | Loss: 0.4862\n",
      "Epoch 00033 | Loss: 0.4846\n",
      "Epoch 00034 | Loss: 0.4833\n",
      "Epoch 00035 | Loss: 0.4817\n",
      "Epoch 00036 | Loss: 0.4800\n",
      "F1-Score: 0.5515\n",
      "Epoch 00037 | Loss: 0.4784\n",
      "Epoch 00038 | Loss: 0.4768\n",
      "Epoch 00039 | Loss: 0.4756\n",
      "Epoch 00040 | Loss: 0.4746\n",
      "Epoch 00041 | Loss: 0.4740\n",
      "F1-Score: 0.5581\n",
      "Epoch 00042 | Loss: 0.4734\n",
      "Epoch 00043 | Loss: 0.4724\n",
      "Epoch 00044 | Loss: 0.4703\n",
      "Epoch 00045 | Loss: 0.4689\n",
      "Epoch 00046 | Loss: 0.4683\n",
      "F1-Score: 0.5550\n",
      "Epoch 00047 | Loss: 0.4671\n",
      "Epoch 00048 | Loss: 0.4646\n",
      "Epoch 00049 | Loss: 0.4638\n",
      "Epoch 00050 | Loss: 0.4622\n",
      "Epoch 00051 | Loss: 0.4620\n",
      "F1-Score: 0.5687\n",
      "Epoch 00052 | Loss: 0.4597\n",
      "Epoch 00053 | Loss: 0.4584\n",
      "Epoch 00054 | Loss: 0.4572\n",
      "Epoch 00055 | Loss: 0.4561\n",
      "Epoch 00056 | Loss: 0.4550\n",
      "F1-Score: 0.5891\n",
      "Epoch 00057 | Loss: 0.4549\n",
      "Epoch 00058 | Loss: 0.4533\n",
      "Epoch 00059 | Loss: 0.4522\n",
      "Epoch 00060 | Loss: 0.4508\n",
      "Epoch 00061 | Loss: 0.4507\n",
      "F1-Score: 0.5877\n",
      "Epoch 00062 | Loss: 0.4496\n",
      "Epoch 00063 | Loss: 0.4489\n",
      "Epoch 00064 | Loss: 0.4479\n",
      "Epoch 00065 | Loss: 0.4476\n",
      "Epoch 00066 | Loss: 0.4480\n",
      "F1-Score: 0.5818\n",
      "Epoch 00067 | Loss: 0.4476\n",
      "Epoch 00068 | Loss: 0.4471\n",
      "Epoch 00069 | Loss: 0.4479\n",
      "Epoch 00070 | Loss: 0.4471\n",
      "Epoch 00071 | Loss: 0.4465\n",
      "F1-Score: 0.5971\n",
      "Epoch 00072 | Loss: 0.4464\n",
      "Epoch 00073 | Loss: 0.4454\n",
      "Epoch 00074 | Loss: 0.4447\n",
      "Epoch 00075 | Loss: 0.4455\n",
      "Epoch 00076 | Loss: 0.4444\n",
      "F1-Score: 0.5773\n",
      "Epoch 00077 | Loss: 0.4425\n",
      "Epoch 00078 | Loss: 0.4419\n",
      "Epoch 00079 | Loss: 0.4413\n",
      "Epoch 00080 | Loss: 0.4397\n",
      "Epoch 00081 | Loss: 0.4396\n",
      "F1-Score: 0.5678\n",
      "Epoch 00082 | Loss: 0.4398\n",
      "Epoch 00083 | Loss: 0.4401\n",
      "Epoch 00084 | Loss: 0.4419\n",
      "Epoch 00085 | Loss: 0.4418\n",
      "Epoch 00086 | Loss: 0.4388\n",
      "F1-Score: 0.5830\n",
      "Epoch 00087 | Loss: 0.4370\n",
      "Epoch 00088 | Loss: 0.4353\n",
      "Epoch 00089 | Loss: 0.4345\n",
      "Epoch 00090 | Loss: 0.4330\n",
      "Epoch 00091 | Loss: 0.4317\n",
      "F1-Score: 0.5961\n",
      "Epoch 00092 | Loss: 0.4308\n",
      "Epoch 00093 | Loss: 0.4293\n",
      "Epoch 00094 | Loss: 0.4287\n",
      "Epoch 00095 | Loss: 0.4287\n",
      "Epoch 00096 | Loss: 0.4275\n",
      "F1-Score: 0.6030\n",
      "Epoch 00097 | Loss: 0.4272\n",
      "Epoch 00098 | Loss: 0.4273\n",
      "Epoch 00099 | Loss: 0.4287\n",
      "Epoch 00100 | Loss: 0.4276\n",
      "Epoch 00101 | Loss: 0.4268\n",
      "F1-Score: 0.6163\n",
      "Epoch 00102 | Loss: 0.4265\n",
      "Epoch 00103 | Loss: 0.4251\n",
      "Epoch 00104 | Loss: 0.4255\n",
      "Epoch 00105 | Loss: 0.4258\n",
      "Epoch 00106 | Loss: 0.4250\n",
      "F1-Score: 0.6019\n",
      "Epoch 00107 | Loss: 0.4245\n",
      "Epoch 00108 | Loss: 0.4242\n",
      "Epoch 00109 | Loss: 0.4233\n",
      "Epoch 00110 | Loss: 0.4237\n",
      "Epoch 00111 | Loss: 0.4237\n",
      "F1-Score: 0.5960\n",
      "Epoch 00112 | Loss: 0.4226\n",
      "Epoch 00113 | Loss: 0.4221\n",
      "Epoch 00114 | Loss: 0.4221\n",
      "Epoch 00115 | Loss: 0.4222\n",
      "Epoch 00116 | Loss: 0.4213\n",
      "F1-Score: 0.6295\n",
      "Epoch 00117 | Loss: 0.4219\n",
      "Epoch 00118 | Loss: 0.4222\n",
      "Epoch 00119 | Loss: 0.4218\n",
      "Epoch 00120 | Loss: 0.4215\n",
      "Epoch 00121 | Loss: 0.4217\n",
      "F1-Score: 0.6021\n",
      "Epoch 00122 | Loss: 0.4221\n",
      "Epoch 00123 | Loss: 0.4206\n",
      "Epoch 00124 | Loss: 0.4222\n",
      "Epoch 00125 | Loss: 0.4198\n",
      "Epoch 00126 | Loss: 0.4187\n",
      "F1-Score: 0.6336\n",
      "Epoch 00127 | Loss: 0.4193\n",
      "Epoch 00128 | Loss: 0.4171\n",
      "Epoch 00129 | Loss: 0.4161\n",
      "Epoch 00130 | Loss: 0.4163\n",
      "Epoch 00131 | Loss: 0.4159\n",
      "F1-Score: 0.6162\n",
      "Epoch 00132 | Loss: 0.4161\n",
      "Epoch 00133 | Loss: 0.4177\n",
      "Epoch 00134 | Loss: 0.4171\n",
      "Epoch 00135 | Loss: 0.4183\n",
      "Epoch 00136 | Loss: 0.4184\n",
      "F1-Score: 0.5833\n",
      "Epoch 00137 | Loss: 0.4187\n",
      "Epoch 00138 | Loss: 0.4196\n",
      "Epoch 00139 | Loss: 0.4188\n",
      "Epoch 00140 | Loss: 0.4163\n",
      "Epoch 00141 | Loss: 0.4163\n",
      "F1-Score: 0.5928\n",
      "Epoch 00142 | Loss: 0.4153\n",
      "Epoch 00143 | Loss: 0.4136\n",
      "Epoch 00144 | Loss: 0.4153\n",
      "Epoch 00145 | Loss: 0.4145\n",
      "Epoch 00146 | Loss: 0.4169\n",
      "F1-Score: 0.6298\n",
      "Epoch 00147 | Loss: 0.4184\n",
      "Epoch 00148 | Loss: 0.4199\n",
      "Epoch 00149 | Loss: 0.4225\n",
      "Epoch 00150 | Loss: 0.4222\n",
      "Epoch 00151 | Loss: 0.4234\n",
      "F1-Score: 0.6385\n",
      "Epoch 00152 | Loss: 0.4286\n",
      "Epoch 00153 | Loss: 0.4244\n",
      "Epoch 00154 | Loss: 0.4213\n",
      "Epoch 00155 | Loss: 0.4201\n",
      "Epoch 00156 | Loss: 0.4167\n",
      "F1-Score: 0.6300\n",
      "Epoch 00157 | Loss: 0.4182\n",
      "Epoch 00158 | Loss: 0.4166\n",
      "Epoch 00159 | Loss: 0.4172\n",
      "Epoch 00160 | Loss: 0.4147\n",
      "Epoch 00161 | Loss: 0.4141\n",
      "F1-Score: 0.6081\n",
      "Epoch 00162 | Loss: 0.4122\n",
      "Epoch 00163 | Loss: 0.4122\n",
      "Epoch 00164 | Loss: 0.4120\n",
      "Epoch 00165 | Loss: 0.4109\n",
      "Epoch 00166 | Loss: 0.4121\n",
      "F1-Score: 0.6012\n",
      "Epoch 00167 | Loss: 0.4094\n",
      "Epoch 00168 | Loss: 0.4104\n",
      "Epoch 00169 | Loss: 0.4097\n",
      "Epoch 00170 | Loss: 0.4089\n",
      "Epoch 00171 | Loss: 0.4066\n",
      "F1-Score: 0.6233\n",
      "Epoch 00172 | Loss: 0.4056\n",
      "Epoch 00173 | Loss: 0.4041\n",
      "Epoch 00174 | Loss: 0.4040\n",
      "Epoch 00175 | Loss: 0.4030\n",
      "Epoch 00176 | Loss: 0.4024\n",
      "F1-Score: 0.6490\n",
      "Epoch 00177 | Loss: 0.4032\n",
      "Epoch 00178 | Loss: 0.4036\n",
      "Epoch 00179 | Loss: 0.4040\n",
      "Epoch 00180 | Loss: 0.4040\n",
      "Epoch 00181 | Loss: 0.4058\n",
      "F1-Score: 0.6556\n",
      "Epoch 00182 | Loss: 0.4074\n",
      "Epoch 00183 | Loss: 0.4085\n",
      "Epoch 00184 | Loss: 0.4070\n",
      "Epoch 00185 | Loss: 0.4072\n",
      "Epoch 00186 | Loss: 0.4064\n",
      "F1-Score: 0.6421\n",
      "Epoch 00187 | Loss: 0.4057\n",
      "Epoch 00188 | Loss: 0.4031\n",
      "Epoch 00189 | Loss: 0.4000\n",
      "Epoch 00190 | Loss: 0.3983\n",
      "Epoch 00191 | Loss: 0.3971\n",
      "F1-Score: 0.6380\n",
      "Epoch 00192 | Loss: 0.3966\n",
      "Epoch 00193 | Loss: 0.3958\n",
      "Epoch 00194 | Loss: 0.3950\n",
      "Epoch 00195 | Loss: 0.3951\n",
      "Epoch 00196 | Loss: 0.3951\n",
      "F1-Score: 0.6507\n",
      "Epoch 00197 | Loss: 0.3954\n",
      "Epoch 00198 | Loss: 0.3959\n",
      "Epoch 00199 | Loss: 0.3968\n",
      "Epoch 00200 | Loss: 0.3978\n",
      "Epoch 00201 | Loss: 0.3987\n",
      "F1-Score: 0.6447\n",
      "Epoch 00202 | Loss: 0.3979\n",
      "Epoch 00203 | Loss: 0.3965\n",
      "Epoch 00204 | Loss: 0.3956\n",
      "Epoch 00205 | Loss: 0.3959\n",
      "Epoch 00206 | Loss: 0.3956\n",
      "F1-Score: 0.6280\n",
      "Epoch 00207 | Loss: 0.3946\n",
      "Epoch 00208 | Loss: 0.3954\n",
      "Epoch 00209 | Loss: 0.3952\n",
      "Epoch 00210 | Loss: 0.3960\n",
      "Epoch 00211 | Loss: 0.4003\n",
      "F1-Score: 0.6150\n",
      "Epoch 00212 | Loss: 0.4003\n",
      "Epoch 00213 | Loss: 0.3996\n",
      "Epoch 00214 | Loss: 0.4013\n",
      "Epoch 00215 | Loss: 0.4031\n",
      "Epoch 00216 | Loss: 0.4004\n",
      "F1-Score: 0.6502\n",
      "Epoch 00217 | Loss: 0.4029\n",
      "Epoch 00218 | Loss: 0.4035\n",
      "Epoch 00219 | Loss: 0.4012\n",
      "Epoch 00220 | Loss: 0.4065\n",
      "Epoch 00221 | Loss: 0.4117\n",
      "F1-Score: 0.6578\n",
      "Epoch 00222 | Loss: 0.4179\n",
      "Epoch 00223 | Loss: 0.4329\n",
      "Epoch 00224 | Loss: 0.4236\n",
      "Epoch 00225 | Loss: 0.4183\n",
      "Epoch 00226 | Loss: 0.4144\n",
      "F1-Score: 0.6372\n",
      "Epoch 00227 | Loss: 0.4141\n",
      "Epoch 00228 | Loss: 0.4114\n",
      "Epoch 00229 | Loss: 0.4064\n",
      "Epoch 00230 | Loss: 0.4024\n",
      "Epoch 00231 | Loss: 0.4000\n",
      "F1-Score: 0.6166\n",
      "Epoch 00232 | Loss: 0.3975\n",
      "Epoch 00233 | Loss: 0.3956\n",
      "Epoch 00234 | Loss: 0.3959\n",
      "Epoch 00235 | Loss: 0.3963\n",
      "Epoch 00236 | Loss: 0.3957\n",
      "F1-Score: 0.6213\n",
      "Epoch 00237 | Loss: 0.3960\n",
      "Epoch 00238 | Loss: 0.3953\n",
      "Epoch 00239 | Loss: 0.3958\n",
      "Epoch 00240 | Loss: 0.3979\n",
      "Epoch 00241 | Loss: 0.3977\n",
      "F1-Score: 0.6200\n",
      "Epoch 00242 | Loss: 0.3986\n",
      "Epoch 00243 | Loss: 0.3968\n",
      "Epoch 00244 | Loss: 0.3953\n",
      "Epoch 00245 | Loss: 0.3942\n",
      "Epoch 00246 | Loss: 0.3934\n",
      "F1-Score: 0.6328\n",
      "Epoch 00247 | Loss: 0.3920\n",
      "Epoch 00248 | Loss: 0.3910\n",
      "Epoch 00249 | Loss: 0.3910\n",
      "Epoch 00250 | Loss: 0.3901\n",
      "Epoch 00251 | Loss: 0.3889\n",
      "F1-Score: 0.6518\n",
      "Epoch 00252 | Loss: 0.3886\n",
      "Epoch 00253 | Loss: 0.3879\n",
      "Epoch 00254 | Loss: 0.3872\n",
      "Epoch 00255 | Loss: 0.3875\n",
      "Epoch 00256 | Loss: 0.3888\n",
      "F1-Score: 0.6669\n",
      "Epoch 00257 | Loss: 0.3907\n",
      "Epoch 00258 | Loss: 0.3927\n",
      "Epoch 00259 | Loss: 0.3946\n",
      "Epoch 00260 | Loss: 0.3966\n",
      "Epoch 00261 | Loss: 0.3978\n",
      "F1-Score: 0.6657\n",
      "Epoch 00262 | Loss: 0.3981\n",
      "Epoch 00263 | Loss: 0.3983\n",
      "Epoch 00264 | Loss: 0.3939\n",
      "Epoch 00265 | Loss: 0.3913\n",
      "Epoch 00266 | Loss: 0.3892\n",
      "F1-Score: 0.6536\n",
      "Epoch 00267 | Loss: 0.3883\n",
      "Epoch 00268 | Loss: 0.3866\n",
      "Epoch 00269 | Loss: 0.3868\n",
      "Epoch 00270 | Loss: 0.3900\n",
      "Epoch 00271 | Loss: 0.3893\n",
      "F1-Score: 0.6220\n",
      "Epoch 00272 | Loss: 0.3919\n",
      "Epoch 00273 | Loss: 0.3960\n",
      "Epoch 00274 | Loss: 0.4000\n",
      "Epoch 00275 | Loss: 0.4023\n",
      "Epoch 00276 | Loss: 0.4040\n",
      "F1-Score: 0.6175\n",
      "Epoch 00277 | Loss: 0.4039\n",
      "Epoch 00278 | Loss: 0.3996\n",
      "Epoch 00279 | Loss: 0.3963\n",
      "Epoch 00280 | Loss: 0.3948\n",
      "Epoch 00281 | Loss: 0.3920\n",
      "F1-Score: 0.6416\n",
      "Epoch 00282 | Loss: 0.3908\n",
      "Epoch 00283 | Loss: 0.3892\n",
      "Epoch 00284 | Loss: 0.3901\n",
      "Epoch 00285 | Loss: 0.3880\n",
      "Epoch 00286 | Loss: 0.3885\n",
      "F1-Score: 0.6708\n",
      "Epoch 00287 | Loss: 0.3872\n",
      "Epoch 00288 | Loss: 0.3876\n",
      "Epoch 00289 | Loss: 0.3889\n",
      "Epoch 00290 | Loss: 0.3882\n",
      "Epoch 00291 | Loss: 0.3896\n",
      "F1-Score: 0.6675\n",
      "Epoch 00292 | Loss: 0.3904\n",
      "Epoch 00293 | Loss: 0.3927\n",
      "Epoch 00294 | Loss: 0.3922\n",
      "Epoch 00295 | Loss: 0.3936\n",
      "Epoch 00296 | Loss: 0.3899\n",
      "F1-Score: 0.6686\n",
      "Epoch 00297 | Loss: 0.3889\n",
      "Epoch 00298 | Loss: 0.3889\n",
      "Epoch 00299 | Loss: 0.3873\n",
      "Epoch 00300 | Loss: 0.3849\n",
      "Epoch 00301 | Loss: 0.3835\n",
      "F1-Score: 0.6715\n",
      "Epoch 00302 | Loss: 0.3831\n",
      "Epoch 00303 | Loss: 0.3826\n",
      "Epoch 00304 | Loss: 0.3818\n",
      "Epoch 00305 | Loss: 0.3808\n",
      "Epoch 00306 | Loss: 0.3799\n",
      "F1-Score: 0.6658\n",
      "Epoch 00307 | Loss: 0.3795\n",
      "Epoch 00308 | Loss: 0.3789\n",
      "Epoch 00309 | Loss: 0.3782\n",
      "Epoch 00310 | Loss: 0.3784\n",
      "Epoch 00311 | Loss: 0.3785\n",
      "F1-Score: 0.6436\n",
      "Epoch 00312 | Loss: 0.3788\n",
      "Epoch 00313 | Loss: 0.3802\n",
      "Epoch 00314 | Loss: 0.3812\n",
      "Epoch 00315 | Loss: 0.3834\n",
      "Epoch 00316 | Loss: 0.3861\n",
      "F1-Score: 0.6350\n",
      "Epoch 00317 | Loss: 0.3888\n",
      "Epoch 00318 | Loss: 0.3880\n",
      "Epoch 00319 | Loss: 0.3890\n",
      "Epoch 00320 | Loss: 0.3898\n",
      "Epoch 00321 | Loss: 0.3887\n",
      "F1-Score: 0.6589\n",
      "Epoch 00322 | Loss: 0.3883\n",
      "Epoch 00323 | Loss: 0.3871\n",
      "Epoch 00324 | Loss: 0.3847\n",
      "Epoch 00325 | Loss: 0.3817\n",
      "Epoch 00326 | Loss: 0.3801\n",
      "F1-Score: 0.6692\n",
      "Epoch 00327 | Loss: 0.3784\n",
      "Epoch 00328 | Loss: 0.3773\n",
      "Epoch 00329 | Loss: 0.3765\n",
      "Epoch 00330 | Loss: 0.3755\n",
      "Epoch 00331 | Loss: 0.3746\n",
      "F1-Score: 0.6659\n",
      "Epoch 00332 | Loss: 0.3745\n",
      "Epoch 00333 | Loss: 0.3748\n",
      "Epoch 00334 | Loss: 0.3754\n",
      "Epoch 00335 | Loss: 0.3770\n",
      "Epoch 00336 | Loss: 0.3791\n",
      "F1-Score: 0.6613\n",
      "Epoch 00337 | Loss: 0.3815\n",
      "Epoch 00338 | Loss: 0.3818\n",
      "Epoch 00339 | Loss: 0.3832\n",
      "Epoch 00340 | Loss: 0.3809\n",
      "Epoch 00341 | Loss: 0.3801\n",
      "F1-Score: 0.6592\n",
      "Epoch 00342 | Loss: 0.3773\n",
      "Epoch 00343 | Loss: 0.3749\n",
      "Epoch 00344 | Loss: 0.3732\n",
      "Epoch 00345 | Loss: 0.3733\n",
      "Epoch 00346 | Loss: 0.3741\n",
      "F1-Score: 0.6857\n",
      "Epoch 00347 | Loss: 0.3737\n",
      "Epoch 00348 | Loss: 0.3775\n",
      "Epoch 00349 | Loss: 0.3822\n",
      "Epoch 00350 | Loss: 0.3901\n",
      "Epoch 00351 | Loss: 0.4032\n",
      "F1-Score: 0.5895\n",
      "Epoch 00352 | Loss: 0.3998\n",
      "Epoch 00353 | Loss: 0.4043\n",
      "Epoch 00354 | Loss: 0.3998\n",
      "Epoch 00355 | Loss: 0.3959\n",
      "Epoch 00356 | Loss: 0.3908\n",
      "F1-Score: 0.6453\n",
      "Epoch 00357 | Loss: 0.3905\n",
      "Epoch 00358 | Loss: 0.3838\n",
      "Epoch 00359 | Loss: 0.3814\n",
      "Epoch 00360 | Loss: 0.3801\n",
      "Epoch 00361 | Loss: 0.3775\n",
      "F1-Score: 0.6570\n",
      "Epoch 00362 | Loss: 0.3777\n",
      "Epoch 00363 | Loss: 0.3748\n",
      "Epoch 00364 | Loss: 0.3738\n",
      "Epoch 00365 | Loss: 0.3718\n",
      "Epoch 00366 | Loss: 0.3711\n",
      "F1-Score: 0.6723\n",
      "Epoch 00367 | Loss: 0.3699\n",
      "Epoch 00368 | Loss: 0.3686\n",
      "Epoch 00369 | Loss: 0.3683\n",
      "Epoch 00370 | Loss: 0.3678\n",
      "Epoch 00371 | Loss: 0.3681\n",
      "F1-Score: 0.6606\n",
      "Epoch 00372 | Loss: 0.3682\n",
      "Epoch 00373 | Loss: 0.3701\n",
      "Epoch 00374 | Loss: 0.3722\n",
      "Epoch 00375 | Loss: 0.3740\n",
      "Epoch 00376 | Loss: 0.3757\n",
      "F1-Score: 0.6713\n",
      "Epoch 00377 | Loss: 0.3771\n",
      "Epoch 00378 | Loss: 0.3769\n",
      "Epoch 00379 | Loss: 0.3769\n",
      "Epoch 00380 | Loss: 0.3756\n",
      "Epoch 00381 | Loss: 0.3761\n",
      "F1-Score: 0.6631\n",
      "Epoch 00382 | Loss: 0.3751\n",
      "Epoch 00383 | Loss: 0.3734\n",
      "Epoch 00384 | Loss: 0.3731\n",
      "Epoch 00385 | Loss: 0.3758\n",
      "Epoch 00386 | Loss: 0.3804\n",
      "F1-Score: 0.6849\n",
      "Epoch 00387 | Loss: 0.3838\n",
      "Epoch 00388 | Loss: 0.3899\n",
      "Epoch 00389 | Loss: 0.3951\n",
      "Epoch 00390 | Loss: 0.4023\n",
      "Epoch 00391 | Loss: 0.3944\n",
      "F1-Score: 0.6430\n",
      "Epoch 00392 | Loss: 0.3904\n",
      "Epoch 00393 | Loss: 0.3873\n",
      "Epoch 00394 | Loss: 0.3845\n",
      "Epoch 00395 | Loss: 0.3851\n",
      "Epoch 00396 | Loss: 0.3781\n",
      "F1-Score: 0.6541\n",
      "Epoch 00397 | Loss: 0.3746\n",
      "Epoch 00398 | Loss: 0.3722\n",
      "Epoch 00399 | Loss: 0.3695\n",
      "Epoch 00400 | Loss: 0.3686\n",
      "Epoch 00401 | Loss: 0.3664\n",
      "F1-Score: 0.6767\n",
      "Epoch 00402 | Loss: 0.3651\n",
      "Epoch 00403 | Loss: 0.3645\n",
      "Epoch 00404 | Loss: 0.3640\n",
      "Epoch 00405 | Loss: 0.3633\n",
      "Epoch 00406 | Loss: 0.3632\n",
      "F1-Score: 0.6760\n",
      "Epoch 00407 | Loss: 0.3630\n",
      "Epoch 00408 | Loss: 0.3627\n",
      "Epoch 00409 | Loss: 0.3632\n",
      "Epoch 00410 | Loss: 0.3642\n",
      "Epoch 00411 | Loss: 0.3652\n",
      "F1-Score: 0.6847\n",
      "Epoch 00412 | Loss: 0.3670\n",
      "Epoch 00413 | Loss: 0.3693\n",
      "Epoch 00414 | Loss: 0.3708\n",
      "Epoch 00415 | Loss: 0.3707\n",
      "Epoch 00416 | Loss: 0.3734\n",
      "F1-Score: 0.6721\n",
      "Epoch 00417 | Loss: 0.3747\n",
      "Epoch 00418 | Loss: 0.3758\n",
      "Epoch 00419 | Loss: 0.3774\n",
      "Epoch 00420 | Loss: 0.3809\n",
      "Epoch 00421 | Loss: 0.3858\n",
      "F1-Score: 0.6296\n",
      "Epoch 00422 | Loss: 0.3840\n",
      "Epoch 00423 | Loss: 0.3817\n",
      "Epoch 00424 | Loss: 0.3831\n",
      "Epoch 00425 | Loss: 0.3845\n",
      "Epoch 00426 | Loss: 0.3814\n",
      "F1-Score: 0.6579\n",
      "Epoch 00427 | Loss: 0.3819\n",
      "Epoch 00428 | Loss: 0.3766\n",
      "Epoch 00429 | Loss: 0.3768\n",
      "Epoch 00430 | Loss: 0.3764\n",
      "Epoch 00431 | Loss: 0.3716\n",
      "F1-Score: 0.6593\n",
      "Epoch 00432 | Loss: 0.3713\n",
      "Epoch 00433 | Loss: 0.3669\n",
      "Epoch 00434 | Loss: 0.3668\n",
      "Epoch 00435 | Loss: 0.3646\n",
      "Epoch 00436 | Loss: 0.3641\n",
      "F1-Score: 0.6846\n",
      "Epoch 00437 | Loss: 0.3637\n",
      "Epoch 00438 | Loss: 0.3633\n",
      "Epoch 00439 | Loss: 0.3637\n",
      "Epoch 00440 | Loss: 0.3653\n",
      "Epoch 00441 | Loss: 0.3661\n",
      "F1-Score: 0.6534\n",
      "Epoch 00442 | Loss: 0.3672\n",
      "Epoch 00443 | Loss: 0.3686\n",
      "Epoch 00444 | Loss: 0.3710\n",
      "Epoch 00445 | Loss: 0.3731\n",
      "Epoch 00446 | Loss: 0.3746\n",
      "F1-Score: 0.6871\n",
      "Epoch 00447 | Loss: 0.3785\n",
      "Epoch 00448 | Loss: 0.3824\n",
      "Epoch 00449 | Loss: 0.3798\n",
      "Epoch 00450 | Loss: 0.3769\n",
      "Epoch 00451 | Loss: 0.3762\n",
      "F1-Score: 0.6869\n",
      "Epoch 00452 | Loss: 0.3716\n",
      "Epoch 00453 | Loss: 0.3713\n",
      "Epoch 00454 | Loss: 0.3698\n",
      "Epoch 00455 | Loss: 0.3700\n",
      "Epoch 00456 | Loss: 0.3703\n",
      "F1-Score: 0.6887\n",
      "Epoch 00457 | Loss: 0.3677\n",
      "Epoch 00458 | Loss: 0.3679\n",
      "Epoch 00459 | Loss: 0.3655\n",
      "Epoch 00460 | Loss: 0.3649\n",
      "Epoch 00461 | Loss: 0.3640\n",
      "F1-Score: 0.6902\n",
      "Epoch 00462 | Loss: 0.3636\n",
      "Epoch 00463 | Loss: 0.3636\n",
      "Epoch 00464 | Loss: 0.3633\n",
      "Epoch 00465 | Loss: 0.3625\n",
      "Epoch 00466 | Loss: 0.3610\n",
      "F1-Score: 0.6853\n",
      "Epoch 00467 | Loss: 0.3618\n",
      "Epoch 00468 | Loss: 0.3617\n",
      "Epoch 00469 | Loss: 0.3623\n",
      "Epoch 00470 | Loss: 0.3641\n",
      "Epoch 00471 | Loss: 0.3643\n",
      "F1-Score: 0.6756\n",
      "Epoch 00472 | Loss: 0.3635\n",
      "Epoch 00473 | Loss: 0.3654\n",
      "Epoch 00474 | Loss: 0.3648\n",
      "Epoch 00475 | Loss: 0.3685\n",
      "Epoch 00476 | Loss: 0.3723\n",
      "F1-Score: 0.6423\n",
      "Epoch 00477 | Loss: 0.3753\n",
      "Epoch 00478 | Loss: 0.3759\n",
      "Epoch 00479 | Loss: 0.3810\n",
      "Epoch 00480 | Loss: 0.3827\n",
      "Epoch 00481 | Loss: 0.3821\n",
      "F1-Score: 0.6716\n",
      "Epoch 00482 | Loss: 0.3812\n",
      "Epoch 00483 | Loss: 0.3824\n",
      "Epoch 00484 | Loss: 0.3738\n",
      "Epoch 00485 | Loss: 0.3723\n",
      "Epoch 00486 | Loss: 0.3692\n",
      "F1-Score: 0.6844\n",
      "Epoch 00487 | Loss: 0.3666\n",
      "Epoch 00488 | Loss: 0.3673\n",
      "Epoch 00489 | Loss: 0.3652\n",
      "Epoch 00490 | Loss: 0.3638\n",
      "Epoch 00491 | Loss: 0.3627\n",
      "F1-Score: 0.6902\n",
      "Epoch 00492 | Loss: 0.3610\n",
      "Epoch 00493 | Loss: 0.3611\n",
      "Epoch 00494 | Loss: 0.3600\n",
      "Epoch 00495 | Loss: 0.3589\n",
      "Epoch 00496 | Loss: 0.3590\n",
      "F1-Score: 0.6918\n",
      "Epoch 00497 | Loss: 0.3591\n",
      "Epoch 00498 | Loss: 0.3587\n",
      "Epoch 00499 | Loss: 0.3581\n",
      "Epoch 00500 | Loss: 0.3580\n",
      "Epoch 00501 | Loss: 0.3567\n",
      "F1-Score: 0.6776\n",
      "Epoch 00502 | Loss: 0.3567\n",
      "Epoch 00503 | Loss: 0.3572\n",
      "Epoch 00504 | Loss: 0.3580\n",
      "Epoch 00505 | Loss: 0.3604\n",
      "Epoch 00506 | Loss: 0.3637\n",
      "F1-Score: 0.6586\n",
      "Epoch 00507 | Loss: 0.3687\n",
      "Epoch 00508 | Loss: 0.3691\n",
      "Epoch 00509 | Loss: 0.3728\n",
      "Epoch 00510 | Loss: 0.3745\n",
      "Epoch 00511 | Loss: 0.3745\n",
      "F1-Score: 0.6818\n",
      "Epoch 00512 | Loss: 0.3740\n",
      "Epoch 00513 | Loss: 0.3723\n",
      "Epoch 00514 | Loss: 0.3676\n",
      "Epoch 00515 | Loss: 0.3669\n",
      "Epoch 00516 | Loss: 0.3657\n",
      "F1-Score: 0.6892\n",
      "Epoch 00517 | Loss: 0.3638\n",
      "Epoch 00518 | Loss: 0.3640\n",
      "Epoch 00519 | Loss: 0.3605\n",
      "Epoch 00520 | Loss: 0.3608\n",
      "Epoch 00521 | Loss: 0.3601\n",
      "F1-Score: 0.6789\n",
      "Epoch 00522 | Loss: 0.3590\n",
      "Epoch 00523 | Loss: 0.3577\n",
      "Epoch 00524 | Loss: 0.3574\n",
      "Epoch 00525 | Loss: 0.3569\n",
      "Epoch 00526 | Loss: 0.3563\n",
      "F1-Score: 0.6875\n",
      "Epoch 00527 | Loss: 0.3568\n",
      "Epoch 00528 | Loss: 0.3567\n",
      "Epoch 00529 | Loss: 0.3568\n",
      "Epoch 00530 | Loss: 0.3571\n",
      "Epoch 00531 | Loss: 0.3584\n",
      "F1-Score: 0.6844\n",
      "Epoch 00532 | Loss: 0.3611\n",
      "Epoch 00533 | Loss: 0.3615\n",
      "Epoch 00534 | Loss: 0.3621\n",
      "Epoch 00535 | Loss: 0.3628\n",
      "Epoch 00536 | Loss: 0.3635\n",
      "F1-Score: 0.6549\n",
      "Epoch 00537 | Loss: 0.3633\n",
      "Epoch 00538 | Loss: 0.3660\n",
      "Epoch 00539 | Loss: 0.3649\n",
      "Epoch 00540 | Loss: 0.3670\n",
      "Epoch 00541 | Loss: 0.3646\n",
      "F1-Score: 0.6717\n",
      "Epoch 00542 | Loss: 0.3667\n",
      "Epoch 00543 | Loss: 0.3651\n",
      "Epoch 00544 | Loss: 0.3646\n",
      "Epoch 00545 | Loss: 0.3670\n",
      "Epoch 00546 | Loss: 0.3673\n",
      "F1-Score: 0.6753\n",
      "Epoch 00547 | Loss: 0.3674\n",
      "Epoch 00548 | Loss: 0.3650\n",
      "Epoch 00549 | Loss: 0.3687\n",
      "Epoch 00550 | Loss: 0.3634\n",
      "Epoch 00551 | Loss: 0.3629\n",
      "F1-Score: 0.6959\n",
      "Epoch 00552 | Loss: 0.3607\n",
      "Epoch 00553 | Loss: 0.3597\n",
      "Epoch 00554 | Loss: 0.3604\n",
      "Epoch 00555 | Loss: 0.3597\n",
      "Epoch 00556 | Loss: 0.3579\n",
      "F1-Score: 0.6942\n",
      "Epoch 00557 | Loss: 0.3586\n",
      "Epoch 00558 | Loss: 0.3598\n",
      "Epoch 00559 | Loss: 0.3592\n",
      "Epoch 00560 | Loss: 0.3628\n",
      "Epoch 00561 | Loss: 0.3585\n",
      "F1-Score: 0.6947\n",
      "Epoch 00562 | Loss: 0.3589\n",
      "Epoch 00563 | Loss: 0.3591\n",
      "Epoch 00564 | Loss: 0.3568\n",
      "Epoch 00565 | Loss: 0.3577\n",
      "Epoch 00566 | Loss: 0.3548\n",
      "F1-Score: 0.6954\n",
      "Epoch 00567 | Loss: 0.3550\n",
      "Epoch 00568 | Loss: 0.3542\n",
      "Epoch 00569 | Loss: 0.3532\n",
      "Epoch 00570 | Loss: 0.3528\n",
      "Epoch 00571 | Loss: 0.3519\n",
      "F1-Score: 0.6945\n",
      "Epoch 00572 | Loss: 0.3518\n",
      "Epoch 00573 | Loss: 0.3523\n",
      "Epoch 00574 | Loss: 0.3514\n",
      "Epoch 00575 | Loss: 0.3523\n",
      "Epoch 00576 | Loss: 0.3516\n",
      "F1-Score: 0.6853\n",
      "Epoch 00577 | Loss: 0.3523\n",
      "Epoch 00578 | Loss: 0.3528\n",
      "Epoch 00579 | Loss: 0.3543\n",
      "Epoch 00580 | Loss: 0.3590\n",
      "Epoch 00581 | Loss: 0.3621\n",
      "F1-Score: 0.6541\n",
      "Epoch 00582 | Loss: 0.3633\n",
      "Epoch 00583 | Loss: 0.3654\n",
      "Epoch 00584 | Loss: 0.3657\n",
      "Epoch 00585 | Loss: 0.3643\n",
      "Epoch 00586 | Loss: 0.3636\n",
      "F1-Score: 0.6747\n",
      "Epoch 00587 | Loss: 0.3614\n",
      "Epoch 00588 | Loss: 0.3589\n",
      "Epoch 00589 | Loss: 0.3586\n",
      "Epoch 00590 | Loss: 0.3541\n",
      "Epoch 00591 | Loss: 0.3536\n",
      "F1-Score: 0.6944\n",
      "Epoch 00592 | Loss: 0.3506\n",
      "Epoch 00593 | Loss: 0.3491\n",
      "Epoch 00594 | Loss: 0.3488\n",
      "Epoch 00595 | Loss: 0.3491\n",
      "Epoch 00596 | Loss: 0.3498\n",
      "F1-Score: 0.6620\n",
      "Epoch 00597 | Loss: 0.3511\n",
      "Epoch 00598 | Loss: 0.3552\n",
      "Epoch 00599 | Loss: 0.3593\n",
      "Epoch 00600 | Loss: 0.3655\n",
      "Epoch 00601 | Loss: 0.3665\n",
      "F1-Score: 0.6884\n",
      "Epoch 00602 | Loss: 0.3713\n",
      "Epoch 00603 | Loss: 0.3712\n",
      "Epoch 00604 | Loss: 0.3672\n",
      "Epoch 00605 | Loss: 0.3689\n",
      "Epoch 00606 | Loss: 0.3629\n",
      "F1-Score: 0.6995\n",
      "Epoch 00607 | Loss: 0.3608\n",
      "Epoch 00608 | Loss: 0.3565\n",
      "Epoch 00609 | Loss: 0.3544\n",
      "Epoch 00610 | Loss: 0.3518\n",
      "Epoch 00611 | Loss: 0.3513\n",
      "F1-Score: 0.7026\n",
      "Epoch 00612 | Loss: 0.3524\n",
      "Epoch 00613 | Loss: 0.3529\n",
      "Epoch 00614 | Loss: 0.3541\n",
      "Epoch 00615 | Loss: 0.3556\n",
      "Epoch 00616 | Loss: 0.3544\n",
      "F1-Score: 0.6971\n",
      "Epoch 00617 | Loss: 0.3564\n",
      "Epoch 00618 | Loss: 0.3573\n",
      "Epoch 00619 | Loss: 0.3575\n",
      "Epoch 00620 | Loss: 0.3572\n",
      "Epoch 00621 | Loss: 0.3544\n",
      "F1-Score: 0.6945\n",
      "Epoch 00622 | Loss: 0.3526\n",
      "Epoch 00623 | Loss: 0.3506\n",
      "Epoch 00624 | Loss: 0.3490\n",
      "Epoch 00625 | Loss: 0.3484\n",
      "Epoch 00626 | Loss: 0.3472\n",
      "F1-Score: 0.6965\n",
      "Epoch 00627 | Loss: 0.3479\n",
      "Epoch 00628 | Loss: 0.3475\n",
      "Epoch 00629 | Loss: 0.3478\n",
      "Epoch 00630 | Loss: 0.3502\n",
      "Epoch 00631 | Loss: 0.3511\n",
      "F1-Score: 0.6683\n",
      "Epoch 00632 | Loss: 0.3527\n",
      "Epoch 00633 | Loss: 0.3578\n",
      "Epoch 00634 | Loss: 0.3607\n",
      "Epoch 00635 | Loss: 0.3638\n",
      "Epoch 00636 | Loss: 0.3658\n",
      "F1-Score: 0.6646\n",
      "Epoch 00637 | Loss: 0.3672\n",
      "Epoch 00638 | Loss: 0.3628\n",
      "Epoch 00639 | Loss: 0.3610\n",
      "Epoch 00640 | Loss: 0.3603\n",
      "Epoch 00641 | Loss: 0.3560\n",
      "F1-Score: 0.6814\n",
      "Epoch 00642 | Loss: 0.3568\n",
      "Epoch 00643 | Loss: 0.3531\n",
      "Epoch 00644 | Loss: 0.3531\n",
      "Epoch 00645 | Loss: 0.3523\n",
      "Epoch 00646 | Loss: 0.3543\n",
      "F1-Score: 0.6981\n",
      "Epoch 00647 | Loss: 0.3516\n",
      "Epoch 00648 | Loss: 0.3522\n",
      "Epoch 00649 | Loss: 0.3487\n",
      "Epoch 00650 | Loss: 0.3480\n",
      "Epoch 00651 | Loss: 0.3482\n",
      "F1-Score: 0.7051\n",
      "Epoch 00652 | Loss: 0.3461\n",
      "Epoch 00653 | Loss: 0.3477\n",
      "Epoch 00654 | Loss: 0.3459\n",
      "Epoch 00655 | Loss: 0.3455\n",
      "Epoch 00656 | Loss: 0.3456\n",
      "F1-Score: 0.6899\n",
      "Epoch 00657 | Loss: 0.3459\n",
      "Epoch 00658 | Loss: 0.3477\n",
      "Epoch 00659 | Loss: 0.3508\n",
      "Epoch 00660 | Loss: 0.3503\n",
      "Epoch 00661 | Loss: 0.3530\n",
      "F1-Score: 0.6632\n",
      "Epoch 00662 | Loss: 0.3500\n",
      "Epoch 00663 | Loss: 0.3532\n",
      "Epoch 00664 | Loss: 0.3549\n",
      "Epoch 00665 | Loss: 0.3562\n",
      "Epoch 00666 | Loss: 0.3574\n",
      "F1-Score: 0.6689\n",
      "Epoch 00667 | Loss: 0.3600\n",
      "Epoch 00668 | Loss: 0.3554\n",
      "Epoch 00669 | Loss: 0.3563\n",
      "Epoch 00670 | Loss: 0.3570\n",
      "Epoch 00671 | Loss: 0.3534\n",
      "F1-Score: 0.6883\n",
      "Epoch 00672 | Loss: 0.3531\n",
      "Epoch 00673 | Loss: 0.3504\n",
      "Epoch 00674 | Loss: 0.3494\n",
      "Epoch 00675 | Loss: 0.3492\n",
      "Epoch 00676 | Loss: 0.3479\n",
      "F1-Score: 0.7011\n",
      "Epoch 00677 | Loss: 0.3470\n",
      "Epoch 00678 | Loss: 0.3457\n",
      "Epoch 00679 | Loss: 0.3447\n",
      "Epoch 00680 | Loss: 0.3444\n",
      "Epoch 00681 | Loss: 0.3445\n",
      "F1-Score: 0.7046\n",
      "Epoch 00682 | Loss: 0.3445\n",
      "Epoch 00683 | Loss: 0.3443\n",
      "Epoch 00684 | Loss: 0.3437\n",
      "Epoch 00685 | Loss: 0.3419\n",
      "Epoch 00686 | Loss: 0.3426\n",
      "F1-Score: 0.7059\n",
      "Epoch 00687 | Loss: 0.3423\n",
      "Epoch 00688 | Loss: 0.3425\n",
      "Epoch 00689 | Loss: 0.3447\n",
      "Epoch 00690 | Loss: 0.3432\n",
      "Epoch 00691 | Loss: 0.3446\n",
      "F1-Score: 0.6914\n",
      "Epoch 00692 | Loss: 0.3450\n",
      "Epoch 00693 | Loss: 0.3449\n",
      "Epoch 00694 | Loss: 0.3454\n",
      "Epoch 00695 | Loss: 0.3437\n",
      "Epoch 00696 | Loss: 0.3448\n",
      "F1-Score: 0.6881\n",
      "Epoch 00697 | Loss: 0.3435\n",
      "Epoch 00698 | Loss: 0.3450\n",
      "Epoch 00699 | Loss: 0.3481\n",
      "Epoch 00700 | Loss: 0.3513\n",
      "Epoch 00701 | Loss: 0.3517\n",
      "F1-Score: 0.6883\n",
      "Epoch 00702 | Loss: 0.3537\n",
      "Epoch 00703 | Loss: 0.3537\n",
      "Epoch 00704 | Loss: 0.3514\n",
      "Epoch 00705 | Loss: 0.3518\n",
      "Epoch 00706 | Loss: 0.3510\n",
      "F1-Score: 0.7030\n",
      "Epoch 00707 | Loss: 0.3493\n",
      "Epoch 00708 | Loss: 0.3502\n",
      "Epoch 00709 | Loss: 0.3491\n",
      "Epoch 00710 | Loss: 0.3523\n",
      "Epoch 00711 | Loss: 0.3559\n",
      "F1-Score: 0.6488\n",
      "Epoch 00712 | Loss: 0.3581\n",
      "Epoch 00713 | Loss: 0.3601\n",
      "Epoch 00714 | Loss: 0.3679\n",
      "Epoch 00715 | Loss: 0.3710\n",
      "Epoch 00716 | Loss: 0.3673\n",
      "F1-Score: 0.6791\n",
      "Epoch 00717 | Loss: 0.3711\n",
      "Epoch 00718 | Loss: 0.3624\n",
      "Epoch 00719 | Loss: 0.3584\n",
      "Epoch 00720 | Loss: 0.3546\n",
      "Epoch 00721 | Loss: 0.3511\n",
      "F1-Score: 0.6932\n",
      "Epoch 00722 | Loss: 0.3458\n",
      "Epoch 00723 | Loss: 0.3425\n",
      "Epoch 00724 | Loss: 0.3404\n",
      "Epoch 00725 | Loss: 0.3396\n",
      "Epoch 00726 | Loss: 0.3381\n",
      "F1-Score: 0.7045\n",
      "Epoch 00727 | Loss: 0.3381\n",
      "Epoch 00728 | Loss: 0.3371\n",
      "Epoch 00729 | Loss: 0.3368\n",
      "Epoch 00730 | Loss: 0.3372\n",
      "Epoch 00731 | Loss: 0.3373\n",
      "F1-Score: 0.7012\n",
      "Epoch 00732 | Loss: 0.3382\n",
      "Epoch 00733 | Loss: 0.3399\n",
      "Epoch 00734 | Loss: 0.3408\n",
      "Epoch 00735 | Loss: 0.3422\n",
      "Epoch 00736 | Loss: 0.3442\n",
      "F1-Score: 0.6765\n",
      "Epoch 00737 | Loss: 0.3430\n",
      "Epoch 00738 | Loss: 0.3435\n",
      "Epoch 00739 | Loss: 0.3443\n",
      "Epoch 00740 | Loss: 0.3439\n",
      "Epoch 00741 | Loss: 0.3458\n",
      "F1-Score: 0.7051\n",
      "Epoch 00742 | Loss: 0.3469\n",
      "Epoch 00743 | Loss: 0.3508\n",
      "Epoch 00744 | Loss: 0.3489\n",
      "Epoch 00745 | Loss: 0.3522\n",
      "Epoch 00746 | Loss: 0.3563\n",
      "F1-Score: 0.7061\n"
     ]
    }
   ],
   "source": [
    "### Max number of epochs\n",
    "max_epochs = epoch_list[-1]+1\n",
    "\n",
    "### DEFINE THE MODEL\n",
    "basic_model = BasicGraphModel(  input_size = n_features, \n",
    "                                hidden_size = 256, \n",
    "                                output_size = n_classes).to(device)\n",
    "\n",
    "### DEFINE LOSS FUNCTION\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "### DEFINE OPTIMIZER\n",
    "optimizer = torch.optim.Adam(basic_model.parameters(), lr=0.005)\n",
    "\n",
    "### TRAIN THE MODEL\n",
    "_, basic_model_scores = train(basic_model, loss_fcn, device, optimizer, max_epochs, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWatNTPBpQGY"
   },
   "source": [
    "Let's evaluate the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "WFWMqwDuSj7b",
    "outputId": "2bd99cee-f8f4-4046-c62c-e3e48f271fd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Model : F1-Score on the test set: 0.9703\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_test = evaluate(student_model, loss_fcn, device, test_dataloader)\n",
    "print(\"Student Model : F1-Score on the test set: {:.4f}\".format(score_test))\n",
    "\n",
    "def plot_f1_score(epoch_list, basic_model_scores, student_model_scores) :\n",
    "    plt.figure(figsize = [10,5])\n",
    "    plt.plot(epoch_list, basic_model_scores, 'b', label = \"Basic Model\")\n",
    "    plt.plot(epoch_list, student_model_scores, 'r', label = \"Student Model\")\n",
    "    plt.title(\"Evolution of f1 score w.r.t epochs\")\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.ylabel(\"Epochs\")\n",
    "    plt.xlabel(\"F1-Score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_f1_score(epoch_list, basic_model_scores, student_model_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9aVEYkuR3fp"
   },
   "source": [
    "## **PART 2 : QUESTIONS** (12/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ-r6AWtR-Co"
   },
   "source": [
    "**1. Make a small paragraph that : (4pts)**\n",
    "1. Explains your achitecture and justify your choices (why the Graph Layer you chose is more efficient than the GCNLayer from the Basic Model?).\n",
    "2. Analyses your results (what is the F1-Score ? are your results convincing ? what is your position w.r.t state-of-the-art ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GH5SQUEFeeqY"
   },
   "source": [
    "(ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aiCvW4q5jRY"
   },
   "source": [
    "The architecture proposed is replicated from the paper \\it{Graph Attentional Network} published as a conference paper at ICLR 2018. Thee choice of this architecture is driven by the performances reached in the paper for the PPI dataset.\n",
    "\n",
    "This architecture (GAT) has multiple advantages over GCN architextures, in terms of:\n",
    "\n",
    "* **Expressivity.** First GATConv layer is able to compute more meaningful representation of input nodes. Indeed, GATConv use an attention mechanism to compute attention between a node and its neighborhood in order to compute some coefficients, and use them for representing the node as the sum of itself and is neighboorhood. As a result, the importance of each nodes of the neighborhood is different for each head attention and for each GATConv layer.\n",
    "In GCNLayer the distribution coefficients are not learn, they are given by some operation on adjency matrix and degree matrix. Therefore, these coeffecients are fixed. The coefficients in GCNLayer are fixed and not learned. These coefficients are obtained through some operation on the adjacetncy matrix and degree matrix. GAT architecture's attention mechanism allows for the learning of attention coefficients, providing greater expressiveness.\n",
    "* **Overcoming oversmoothing.** GATs address the limitations of GCNs, which rely on a fixed graph structure and suffer from oversmoothing, by using masked self-attentional layers that can handle varying graph structures and assign different importances to different nodes within a neighborhood.\n",
    "* **Computational power.** Despite attention mechanism adds a  computation of specific coefficient for the neighborhood of each node, the function is highly parrallelisable and the time for computation remains in same order of magnitude as for GCN network with much more expressive functions. Along article, this could be linked to lesser matrix inversion and/or eigenvector decomposition. Moreover, improvements could be later achieved in using more efficiently GPU power along article.\n",
    "\n",
    "Other avantages of the GAT not specifically used here, is that the attention mechanism, once learned, can be applied to graph which do not have the same structure as the ones used for training. Hence, it allows inductive resolution of problems. It leads to a better tractability of the results, and does not require the graph to be directed. \n",
    "\n",
    "The F1-Score is a widely employed measure to evaluate binary classification problems, wherein the said metric is calculated as the harmonic mean of precision and recall. The Formula reads:\n",
    "$$score = 2 \\frac{precision \\times recall}{precision + recall},$$\n",
    "with $$precision = \\frac{true\\ positives}{true\\ positives+ false\\ positives},$$\n",
    "and \n",
    "$$recall = \\frac{true\\ positives}{true\\ positives +false\\ negatives}.$$\n",
    "\n",
    "The resulting score ranges from 0 to 1, where higher values are indicative of superior performance.\n",
    "Our algorithm achieves an F1-score of 0.9703, which is close to 1. The paper reports an F1-score of 0.973, indicating that we are approaching the state-of-the-art performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HIVqG4u5jfc"
   },
   "source": [
    "**2. Provide a diagramm of your architecture, which includes a good and clear legend as well as shapes information. The diagramm must be submitted as an external file, along with this notebook (PDF, JEPG or PNG format accepted). (2pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crMHLjkJ5pxl"
   },
   "source": [
    "**3. Make a small paragraph that explains: (6pts)**\n",
    "1. What _oversmoothing_ is in the context of Graph Neural Network. Why is it an issue ? \n",
    "2. Are there solutions to overcome it ? \n",
    "3. Do you think the model you constructed is robust with respect to oversmoothing ? Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLPosSov_uPK"
   },
   "source": [
    "(ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7jQNMbIegTa"
   },
   "source": [
    "Oversmoothing occurs when the GNN model aggregates information from neighboring nodes through multiple iterations, causing the feature representations of the nodes to become increasingly similar. This makes it difficult for the model to differentiate between nodes and can lead to overgeneralization, which adversely affects the accuracy of the model's predictions. One of the main reasons why oversmoothing is a significant issue is that it can result in the GNN model producing overly confident predictions based on incomplete or incorrect information. This can lead to poor performance in downstream tasks and limit the applicability of GNNs in practical settings.\n",
    "\n",
    "To combat oversmoothing, there has been various approaches proposed in the litterature such as adding skip connections, introducing regularization terms, and using hierarchical aggregation schemes. These methods aim to prevent feature representations from becoming too similar and to maintain the structural information in the graph, thereby improving the overall performance of GNNs.\n",
    "\n",
    "The architecture incorporate those technics to mitigate oversmoothing:\n",
    "* **Attention mechanism.** The GAT model is designed to assign different levels of importance to nodes within a neighborhood, making it less prone to oversmoothing compared to other GNN models that do not consider the distinctiveness of nodes.\n",
    "* **Skip connections.** The architecture incorporates skip connections across the intermediate attentional layer. This is to mitigate the effects of oversmoothing and preserve the integrity of the graph. \n",
    "* **Hierarchical aggregation strategies.** The algorithm also uses hierarchical aggregation strategies, through multi-headed attention layer, which has been proposed to prevent oversmoothing by maintaining node identities and preserving the graph's structural coherence."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
