{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhgZCbQKVudS"
   },
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ERqhrfjVudT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import urllib.request\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJJ59elTVudU"
   },
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `ResNet-34` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **ResNet-34**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9fc42e8262604e2194d3a76d2999869e",
      "ac09705d5d9d47a6bc21bb1fd99e0100",
      "ba64149f4cf544a19a1f729a8cc72281",
      "e3a3dc57cfdd404c8a0a36a871958720",
      "be039df72332445ab23ad8875814ea3d",
      "de52914c134a49fb80e61f8512b220ec",
      "8f9b5704f1e14315a90553efe150bb3a",
      "778e26422a2c46aca4a907b5a02e2fc0",
      "d6f98cf600fc4079aed3c6706db2cb10",
      "4b8c0b7dc898492ca04ed3b9c78938b7",
      "865118b123c04689965ea91fdda5075e"
     ]
    },
    "id": "7fFgyhTPVudU",
    "outputId": "57b56a8f-5848-4b69-9f3f-7b4c6d9eaf73"
   },
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(pretrained=True)\n",
    "resnet34.eval() # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BErHtOzmVudU"
   },
   "source": [
    "![ResNet34](https://miro.medium.com/max/1050/1*Y-u7dH4WC-dXyn9jOG4w0w.png)\n",
    "\n",
    "\n",
    "Input image must be of size (3x224x224). \n",
    "\n",
    "First convolution layer with maxpool. \n",
    "Then 4 ResNet blocks. \n",
    "\n",
    "Output of the last ResNet block is of size (512x7x7). \n",
    "\n",
    "Average pooling is applied to this layer to have a 1D array of 512 features fed to a linear layer that outputs 1000 values (one for each class). No softmax is present in this case. We have already the raw class score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVDZssWmVudU",
    "outputId": "39f89991-019e-4541-d732-67a6d18fd541"
   },
   "outputs": [],
   "source": [
    "classes = pickle.load(urllib.request.urlopen('https://gist.githubusercontent.com/yrevar/6135f1bd8dcf2e0cc683/raw/d133d61a09d7e5a3b36b8c111a8dd5c4b5d560ee/imagenet1000_clsid_to_human.pkl'))\n",
    "\n",
    "##classes is a dictionary with the name of each class \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbnlz7HKVudV"
   },
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course or download directly using the following command line,).<br>\n",
    "In order to use the pretrained model resnet34, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40L6HPDsVudV"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fU9QAkiVudV",
    "outputId": "e5a28d60-2037-4cac-93e7-720ffd54ce81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
    "# otherwise the function won't find them\n",
    "\n",
    "import os\n",
    "os.mkdir(\"data\")\n",
    "os.mkdir(\"data/TP2_images\")\n",
    "!cd data/TP2_images && wget \"https://www.lri.fr/~gcharpia/deeppractice/2023/TP2/TP2_images.zip\" && unzip TP2_images.zip\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "MHln5FbuVudV",
    "outputId": "4d0b545b-e8b2-4cc3-f02b-eaafc7c2438f"
   },
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 0\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MNmQbHsVudW",
    "outputId": "707879b6-a92c-49ca-d31a-cf9f6cc40b27"
   },
   "outputs": [],
   "source": [
    "output = resnet34(dataset[index][0].view(1, 3, 224, 224))\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy(), [classes[x] for x in indices[0].numpy()])\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xSBNvoHVudW"
   },
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **To be submitted within 2 weeks**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model resnet34 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. \n",
    " + The size of feature maps is 7x7, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Awgl0ZL6Vuda"
   },
   "source": [
    "Class: ‘pug, pug-dog’ | Class: ‘tabby, tabby cat’\n",
    "- | - \n",
    "![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/dog.jpg)| ![alt](https://raw.githubusercontent.com/jacobgil/pytorch-grad-cam/master/examples/cat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGPzsqWZXrB2"
   },
   "source": [
    "# Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTlBL2daDy6c"
   },
   "outputs": [],
   "source": [
    "#Function used in the forward and backward hook, to retrieve the gradient and the value output by the layer 4.\n",
    "def get_grad(module,inp,out,list_grad):\n",
    "  return list_grad.append(inp[0].squeeze().numpy())\n",
    "\n",
    "def get_value(module,inp,out,list_value):\n",
    "  return list_value.append(out.detach().squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk8ZOC39FRLz"
   },
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "  def __init__(self,model,classes):\n",
    "    self.model = model\n",
    "    self.classes = classes\n",
    "    self.list_grad = []#list to store the input gradient of the layer 4 for the top 3 classes.\n",
    "    self.list_value =  []#list to store the value output by the layer 4 for the top 3 classes.\n",
    "    self.labels = []#list to keep the label of the top 3 classes.\n",
    "    self.importance_feature = np.zeros((3,512,1,1))#list that will containt the importance coefficients of each feature map.\n",
    "    self.heat_map = np.zeros((3,7,7))\n",
    "    self.hooks = []\n",
    "\n",
    "  def set_hook(self):\n",
    "    self.hooks.append(self.model.layer4.register_backward_hook((lambda module,inp,out : get_grad(module,inp,out,list_grad=self.list_grad))))\n",
    "    self.hooks.append(self.model.layer4.register_forward_hook((lambda module,inp,out : get_value(module,inp,out,list_value=self.list_value))))\n",
    "\n",
    "  def made_map(self,input):\n",
    "    output = resnet34(input)\n",
    "    values, indices = torch.topk(output, 3)\n",
    "    indices = indices.numpy().reshape(-1)\n",
    "    \n",
    "    for i in range(len(indices)) :\n",
    "\n",
    "      indice = indices[i]\n",
    "      value = values[0][i]\n",
    "      self.labels.append(self.classes[indice])#Store the label of the classe indice\n",
    "      \n",
    "      grad = torch.zeros(1,1000)\n",
    "      grad[:,indice] = value\n",
    "      output.backward(grad,retain_graph=True)\n",
    "    \n",
    "    for i in range(len(self.list_grad)):\n",
    "      self.importance_feature[i]=(self.list_grad[i].mean(axis=(1,2)))[:,None,None]#Compute the importance coeffcients of each feature map, according to the formula given during the class. [:,None,None] is to have a array of size (512,1,1).\n",
    "      self.heat_map[i] =(self.importance_feature[i]*self.list_value[0]).sum(axis=0)#Compute the heat map before the interpolation.\n",
    "\n",
    "\n",
    "  def remove_hook(self):\n",
    "    for hook in self.hooks:\n",
    "      hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-eiNUWLujnJ"
   },
   "outputs": [],
   "source": [
    "def Grad_CAM(model,image,input):\n",
    "  img = np.array(Image.open(image).convert('RGB'))\n",
    "  img = np.float32(cv2.resize(img, (224, 224)))/255\n",
    "  input = input.view(1, 3, 224, 224)\n",
    "  \n",
    "  #Build the heatmaps of size 7x7 for the top 3 classes.\n",
    "  grad_cam = GradCam(model,classes)\n",
    "  grad_cam.set_hook()\n",
    "  grad_cam.made_map(input)\n",
    "  grad_cam.remove_hook()\n",
    "\n",
    "  #Interpolate the heatmaps to  get image of size 224x224\n",
    "  heat_map_1 = nn.ReLU()(nn.functional.interpolate(torch.from_numpy((grad_cam.heat_map[0]/np.max(grad_cam.heat_map[0]))[None,None]),size=(224,224),mode='bilinear')).squeeze().numpy()\n",
    "  heat_map_2 = nn.ReLU()(nn.functional.interpolate(torch.from_numpy((grad_cam.heat_map[1]/np.max(grad_cam.heat_map[1]))[None,None]),size=(224,224),mode='bilinear')).squeeze().numpy()\n",
    "  heat_map_3 = nn.ReLU()(nn.functional.interpolate(torch.from_numpy((grad_cam.heat_map[2]/np.max(grad_cam.heat_map[2]))[None,None]),size=(224,224),mode='bilinear')).squeeze().numpy()\n",
    "\n",
    "  #Applied a colormap and ensure that the heatmaps are in RGB format.\n",
    "  heat_map_1 = cv2.applyColorMap(np.uint8(255 * heat_map_1), cv2.COLORMAP_JET)\n",
    "  heat_map_1 = cv2.cvtColor(heat_map_1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  heat_map_2 = cv2.applyColorMap(np.uint8(255 * heat_map_2), cv2.COLORMAP_JET)\n",
    "  heat_map_2 = cv2.cvtColor(heat_map_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  heat_map_3 = cv2.applyColorMap(np.uint8(255 * heat_map_3), cv2.COLORMAP_JET)\n",
    "  heat_map_3 = cv2.cvtColor(heat_map_3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  alpha=0.6\n",
    "  \n",
    "  #Overlay image and heatmaps\n",
    "  plt.figure(figsize=(20,20))\n",
    "  plt.subplot(1,4,1)\n",
    "  plt.imshow(img)\n",
    "  plt.subplot(1,4,2)\n",
    "  plt.title(grad_cam.labels[0].split(',')[0])\n",
    "  plt.imshow(img)\n",
    "  plt.imshow(heat_map_1,alpha=alpha)\n",
    "  plt.subplot(1,4,3)\n",
    "  plt.title(grad_cam.labels[1].split(',')[0])\n",
    "  plt.imshow(img)\n",
    "  plt.imshow(heat_map_2,alpha=alpha)\n",
    "  plt.subplot(1,4,4)\n",
    "  plt.title(grad_cam.labels[2].split(',')[0])\n",
    "  plt.imshow(img)\n",
    "  plt.imshow(heat_map_3,alpha=alpha)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YftTFUrwXR89"
   },
   "source": [
    "## 10 Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9Xoy44EJWuTT",
    "outputId": "82c90f62-c6ce-49a6-9088-8cebb5fe7193"
   },
   "outputs": [],
   "source": [
    "#Compute and display gradcam on the 20 images.\n",
    "for i in range (20):\n",
    "  Grad_CAM(resnet34,dataset.imgs[i][0],dataset[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9Kl6uuAW6pI"
   },
   "source": [
    "##Commented pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "ckTe1Rv1zxq0",
    "outputId": "a5e72a3a-fc7f-4584-911d-d8e5d01b3d9f"
   },
   "outputs": [],
   "source": [
    "Grad_CAM(resnet34,dataset.imgs[2][0],dataset[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8ZUvvgi5HdA"
   },
   "source": [
    "In the three first guesses, one can see that the neural network clearly identifies the position of the dog. The differences lies on the region that were mostly esploited. \n",
    "The third one mainly focused on the head of the dog. This part is indeed very similar to Shih-tzu. \n",
    "The second one focuses mainly on the back of the dog, which is uniformly brown, as it is for old english sheepdog.\n",
    "The first one focuseson the head and the forward body of the dog. Tibetan terrier often has brown fur on the top and white fur on the other parts. Grad-Cam can make us wonder if this is that caracteristic that made it choose this race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "B8103eOH42s1",
    "outputId": "27b3b602-a025-4523-edd1-e7361d7bb3f1"
   },
   "outputs": [],
   "source": [
    "Grad_CAM(resnet34,dataset.imgs[7][0],dataset[7][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5HB95ad7ybk"
   },
   "source": [
    "This example demonstrate the usefullness of such algorithms. One can clearly see that the changes in the ouput come from the object it is looking at: when center around the head of the first dog it is labeled \"great dane\" (image 1), and when it is centered around the head of the second dog it is labeled \"Chesapeake Bay Retriever\" (image 3). But both only look at the head of both dogs, so it is natural that the neural network does not have enough data to make a good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "llI373sOmcZy",
    "outputId": "8eb7bfc2-274d-4dca-81af-721ed27b4a64"
   },
   "outputs": [],
   "source": [
    "Grad_CAM(resnet34,dataset.imgs[15][0],dataset[15][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIxV7AHEmi1i"
   },
   "source": [
    "On this example, we can see that the second highest score is for the \"cowboy boot\" class. Indeed, we can see on the heat map that the activation is maximum in an area that has a kind of \"L\" shape like a cowboy boot. This gives us a good explanation on why the model predicts such a score here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_cVir4roHVy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "4b8c0b7dc898492ca04ed3b9c78938b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "778e26422a2c46aca4a907b5a02e2fc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "865118b123c04689965ea91fdda5075e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f9b5704f1e14315a90553efe150bb3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fc42e8262604e2194d3a76d2999869e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac09705d5d9d47a6bc21bb1fd99e0100",
       "IPY_MODEL_ba64149f4cf544a19a1f729a8cc72281",
       "IPY_MODEL_e3a3dc57cfdd404c8a0a36a871958720"
      ],
      "layout": "IPY_MODEL_be039df72332445ab23ad8875814ea3d"
     }
    },
    "ac09705d5d9d47a6bc21bb1fd99e0100": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de52914c134a49fb80e61f8512b220ec",
      "placeholder": "​",
      "style": "IPY_MODEL_8f9b5704f1e14315a90553efe150bb3a",
      "value": "100%"
     }
    },
    "ba64149f4cf544a19a1f729a8cc72281": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_778e26422a2c46aca4a907b5a02e2fc0",
      "max": 87319819,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6f98cf600fc4079aed3c6706db2cb10",
      "value": 87319819
     }
    },
    "be039df72332445ab23ad8875814ea3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6f98cf600fc4079aed3c6706db2cb10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de52914c134a49fb80e61f8512b220ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3a3dc57cfdd404c8a0a36a871958720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b8c0b7dc898492ca04ed3b9c78938b7",
      "placeholder": "​",
      "style": "IPY_MODEL_865118b123c04689965ea91fdda5075e",
      "value": " 83.3M/83.3M [00:02&lt;00:00, 48.6MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
